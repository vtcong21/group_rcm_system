{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My custom profile for matrix factorization for recommender system\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import os, csv, random\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparametters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=60 # latent factors\n",
    "lam=0.02 # regularization\n",
    "learning_rate=0.001 # learning rate\n",
    "max_iter=200 # max iterations\n",
    "print_every=1 # print loss for each iteration\n",
    "tolerance=1e-6 # tolerance\n",
    "beta=0.4 # min bound to normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create class MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MFOptimized:\n",
    "    def __init__(self, Y: pd.DataFrame, K, lam=0.1, learning_rate=0.01, max_iter=100, print_every=10, tolerance=1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the MF model.\n",
    "        Args:\n",
    "            Y (pandas array): A 2D numpy array of shape (n_users, n_items) representing the rating matrix.\n",
    "                Missing ratings should be represented as 0.\n",
    "            K (int): Number of latent factors.\n",
    "            lam (float): Regularization parameter.\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "            max_iter (int): Number of training iterations.\n",
    "            print_every (int): Print loss every print_every iterations.\n",
    "            tolerance (float): Tolerance for convergence based on change in loss.\n",
    "        \"\"\"\n",
    "        self.Y = Y\n",
    "        self.K = K\n",
    "        self.lam = lam\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.print_every = print_every\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "        # Dimensions of the rating matrix\n",
    "        self.n_users, self.n_items = Y.shape\n",
    "\n",
    "        # Initialize latent factors and biases\n",
    "        self.H = np.random.normal(0, 0.1, (self.n_users, K)).astype(np.float32)  # Latent factors for users\n",
    "        self.Q = np.random.normal(0, 0.1, (self.n_items, K)).astype(np.float32)  # Latent factors for items)\n",
    "        self.o = np.zeros(self.n_users, dtype=np.float32)  # Biases for users\n",
    "        self.p = np.zeros(self.n_items, dtype=np.float32)  # Biases for items\n",
    "        self.mu = np.mean(Y[Y > 0])  # Global average rating (non-zero entries only)\n",
    "\n",
    "    def map_ids_to_indices(self):\n",
    "        \"\"\"\n",
    "        Map original user and movie IDs to their corresponding indices in the pivoted matrix.\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame containing ratings, with userId as index and movieId as columns.\n",
    "        \n",
    "        Returns:\n",
    "            user_id_to_index: Dictionary mapping userId to row index.\n",
    "            movie_id_to_index: Dictionary mapping movieId to column index.\n",
    "            index_to_user_id: Dictionary mapping row index to userId.\n",
    "            index_to_movie_id: Dictionary mapping column index to movieId.\n",
    "        \"\"\"\n",
    "        # Create mappings for userId and movieId\n",
    "        self.user_id_to_index = {user_id: idx for idx, user_id in enumerate(self.Y.index)}\n",
    "        self.movie_id_to_index = {movie_id: idx for idx, movie_id in enumerate(self.Y.columns)}\n",
    "        \n",
    "        # Reverse mappings\n",
    "        self.index_to_user_id = {idx: user_id for user_id, idx in self.user_id_to_index.items()}\n",
    "        self.index_to_movie_id = {idx: movie_id for movie_id, idx in self.movie_id_to_index.items()}\n",
    "        self.Y = self.Y.to_numpy()\n",
    "\n",
    "    def compute_loss(self):\n",
    "        \"\"\"\n",
    "        Compute the loss based on the provided formula.\n",
    "        \"\"\"\n",
    "        mask = self.Y > 0  # Mask to filter out missing ratings\n",
    "        error_sum = 0\n",
    "        regularization_sum = 0\n",
    "\n",
    "        for u in range(self.n_users):\n",
    "            for i in range(self.n_items):\n",
    "                if mask[u, i]:\n",
    "                    r = self.Y[u, i]\n",
    "                    pred = self.o[u] + self.p[i] + self.mu + np.dot(self.H[u], self.Q[i])\n",
    "                    error = r - pred\n",
    "                    error_sum += error ** 2\n",
    "                    regularization_sum += (\n",
    "                        np.sum(self.H[u] ** 2)\n",
    "                        + np.sum(self.Q[i] ** 2)\n",
    "                        + self.o[u] ** 2\n",
    "                        + self.p[i] ** 2\n",
    "                    )\n",
    "\n",
    "        # Compute total loss\n",
    "        loss = 0.5 * error_sum + 0.5 * self.lam * regularization_sum\n",
    "        return loss\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Train the model using stochastic gradient descent (SGD).\n",
    "        \"\"\"\n",
    "        prev_loss = float('inf')\n",
    "\n",
    "        for it in range(self.max_iter):\n",
    "            for u in range(self.n_users):\n",
    "                for i in range(self.n_items):\n",
    "                    if self.Y[u, i] > 0:  # Update only for observed ratings\n",
    "                        r = self.Y[u, i]\n",
    "\n",
    "                        # Calculate prediction\n",
    "                        pred = self.o[u] + self.p[i] + self.mu + np.dot(self.H[u], self.Q[i])\n",
    "\n",
    "                        # Calculate error\n",
    "                        error = r - pred\n",
    "\n",
    "                        # Update latent factors and biases\n",
    "                        self.H[u] += self.learning_rate * (error * self.Q[i] - self.lam * self.H[u])\n",
    "                        self.Q[i] += self.learning_rate * (error * self.H[u] - self.lam * self.Q[i])\n",
    "\n",
    "                        self.o[u] += self.learning_rate * (error - self.lam * self.o[u])\n",
    "                        self.p[i] += self.learning_rate * (error - self.lam * self.p[i])\n",
    "\n",
    "            # Compute current loss and check for convergence\n",
    "            loss = self.compute_loss()\n",
    "\n",
    "            # Check if the change in loss is smaller than the tolerance\n",
    "            if abs(prev_loss - loss) < self.tolerance:\n",
    "                print(f\"Convergence reached at iteration {it + 1}\")\n",
    "                break\n",
    "\n",
    "            prev_loss = loss\n",
    "\n",
    "            # Print loss every 'print_every' iterations\n",
    "            if (it + 1) % self.print_every == 0:\n",
    "                print(f\"Iteration {it + 1}/{self.max_iter}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, u, i):\n",
    "        \"\"\"\n",
    "        Predict the rating for a specific user-item pair.\n",
    "        \"\"\"\n",
    "        u, i = int(u), int(i)\n",
    "        pred = self.o[u] + self.p[i] + self.mu + np.dot(self.H[u], self.Q[i])\n",
    "        return np.clip(pred, 0, 5)\n",
    "\n",
    "\n",
    "    def export_latent_matrices_and_biases(self, output_dir=\"data/output\"):\n",
    "        \"\"\"\n",
    "        Export the latent matrices (H, Q) and biases (o, p) to CSV files.\n",
    "        Args:\n",
    "            output_dir (str): Directory where the files will be saved.\n",
    "        \"\"\" \n",
    "        os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "        # Save user latent factors (H)\n",
    "        np.savetxt(os.path.join(output_dir, \"user_latent_factors.csv\"), self.H, delimiter=\",\")\n",
    "        print(f\"User latent factors saved to {os.path.join(output_dir, 'user_latent_factors.csv')}\")\n",
    "\n",
    "        # Save item latent factors (Q)\n",
    "        np.savetxt(os.path.join(output_dir, \"item_latent_factors.csv\"), self.Q, delimiter=\",\")\n",
    "        print(f\"Item latent factors saved to {os.path.join(output_dir, 'item_latent_factors.csv')}\")\n",
    "\n",
    "        # Save user biases (o)\n",
    "        np.savetxt(os.path.join(output_dir, \"user_biases.csv\"), self.o, delimiter=\",\")\n",
    "        print(f\"User biases saved to {os.path.join(output_dir, 'user_biases.csv')}\")\n",
    "\n",
    "        # Save item biases (p)\n",
    "        np.savetxt(os.path.join(output_dir, \"item_biases.csv\"), self.p, delimiter=\",\")\n",
    "        print(f\"Item biases saved to {os.path.join(output_dir, 'item_biases.csv')}\")\n",
    "\n",
    "        # Save global mean (mu)\n",
    "        with open(os.path.join(output_dir, \"global_mean.txt\"), \"w\") as f:\n",
    "            f.write(str(self.mu))\n",
    "        print(f\"Global mean saved to {os.path.join(output_dir, 'global_mean.txt')}\")\n",
    "\n",
    "    def load_latent_matrices_and_biases(self, input_dir=\"data/output\"):\n",
    "        \"\"\"\n",
    "        Load the latent matrices (H, Q) and biases (o, p) from CSV files.\n",
    "        Args:\n",
    "            input_dir (str): Directory where the files are saved.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load user latent factors (H)\n",
    "            self.H = np.loadtxt(os.path.join(input_dir, \"user_latent_factors.csv\"), delimiter=\",\")\n",
    "            print(f\"User latent factors loaded from {os.path.join(input_dir, 'user_latent_factors.csv')}\")\n",
    "\n",
    "            # Load item latent factors (Q)\n",
    "            self.Q = np.loadtxt(os.path.join(input_dir, \"item_latent_factors.csv\"), delimiter=\",\")\n",
    "            print(f\"Item latent factors loaded from {os.path.join(input_dir, 'item_latent_factors.csv')}\")\n",
    "\n",
    "            # Load user biases (o)\n",
    "            self.o = np.loadtxt(os.path.join(input_dir, \"user_biases.csv\"), delimiter=\",\")\n",
    "            print(f\"User biases loaded from {os.path.join(input_dir, 'user_biases.csv')}\")\n",
    "\n",
    "            # Load item biases (p)\n",
    "            self.p = np.loadtxt(os.path.join(input_dir, \"item_biases.csv\"), delimiter=\",\")\n",
    "            print(f\"Item biases loaded from {os.path.join(input_dir, 'item_biases.csv')}\")\n",
    "\n",
    "            # Load global mean (mu)\n",
    "            with open(os.path.join(input_dir, \"global_mean.txt\"), \"r\") as f:\n",
    "                self.mu = float(f.read())\n",
    "            print(f\"Global mean loaded from {os.path.join(input_dir, 'global_mean.txt')}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading latent matrices and biases: {e}\")\n",
    "\n",
    "    def evaluate(self, threshold=3):\n",
    "        \"\"\"\n",
    "        Evaluate the model using Precision, Recall, and F1 score.\n",
    "        Args:\n",
    "            threshold (int): The rating threshold to consider a \"positive\" prediction.\n",
    "        \"\"\"\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        # Iterate through all users and items\n",
    "        for u in range(self.n_users):\n",
    "            for i in range(self.n_items):\n",
    "                if self.Y[u, i] > 0:  # Only evaluate on observed ratings\n",
    "                    observed_rating = self.Y[u, i]\n",
    "                    predicted_rating = self.predict(u, i)\n",
    "\n",
    "                    # Convert ratings to binary (1 if positive, 0 if negative)\n",
    "                    y_true.append(1 if observed_rating >= threshold else 0)\n",
    "                    y_pred.append(1 if predicted_rating >= threshold else 0)\n",
    "\n",
    "        # Compute Precision, Recall, and F1 Score\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    def make_predicted_matrix(self):\n",
    "        self.predicted_ratings = np.zeros((self.n_users, self.n_items))\n",
    "\n",
    "        for u in range(self.n_users):\n",
    "            for i in range(self.n_items):\n",
    "                # Compute the predicted rating for each user-item pair\n",
    "                prediction = (self.o[u] + self.p[i] + self.mu +\n",
    "                            np.dot(self.H[u], self.Q[i]))\n",
    "                self.predicted_ratings[u, i] = np.clip(prediction, 0, 5)  # Clip to a valid rating range (0 to 5)\n",
    "\n",
    "    def export_ratings(self, output_filename=\"predicted_ratings.csv\"):\n",
    "        \"\"\"\n",
    "        Export the predicted and observed ratings to a CSV file, using original userId and movieId.\n",
    "        Args:\n",
    "            output_filename (str): Name of the output CSV file.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        \n",
    "        for u in range(self.n_users):\n",
    "            for i in range(self.n_items):\n",
    "                if self.Y[u, i] > 0:  # Only consider observed ratings\n",
    "                    observed_rating = self.Y[u, i]\n",
    "                    predicted_rating = self.predict(u, i)\n",
    "                    \n",
    "                    # Map the indices back to original userId and movieId\n",
    "                    original_user_id = self.index_to_user_id[u]\n",
    "                    original_movie_id = self.index_to_movie_id[i]\n",
    "                    \n",
    "                    rows.append([original_user_id, original_movie_id, predicted_rating, observed_rating])\n",
    "\n",
    "        # Create DataFrame and save to CSV\n",
    "        df = pd.DataFrame(rows, columns=[\"userId\", \"movieId\", \"predicted_ratings\", \"observed_ratings\"])\n",
    "        df.to_csv(output_filename, index=False)\n",
    "        print(f\"Predicted ratings saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $c^{[beta,1]}_{u,i}$ matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalized_c_ui(Y, predicted_ratings, user_latent_factors, item_latent_factors, beta=0.4):\n",
    "    \"\"\"\n",
    "    Compute and normalize c_{u,i} for the entire matrix.\n",
    "\n",
    "    Args:\n",
    "        Y (numpy.ndarray): Rating matrix (n_users x n_items), with 0 for missing ratings.\n",
    "        predicted_ratings (numpy.ndarray): Predicted ratings matrix (n_users x n_items).\n",
    "        user_latent_factors (numpy.ndarray): User latent factors matrix (n_users x K).\n",
    "        item_latent_factors (numpy.ndarray): Item latent factors matrix (n_items x K).\n",
    "        beta (float): Minimum bound for normalization (default: 0.4).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Normalized c_{u,i} matrix (n_users x n_items).\n",
    "    \"\"\"\n",
    "    n_users, n_items = Y.shape\n",
    "    c_ui = np.zeros((n_users, n_items), dtype=np.float32)\n",
    "\n",
    "    # Compute raw c_{u,i}\n",
    "    for u in range(n_users):\n",
    "        for i in range(n_items):\n",
    "            if Y[u, i] > 0:  # Observed rating\n",
    "                c_ui[u, i] = 1 - abs(Y[u, i] - predicted_ratings[u, i])\n",
    "            else:  # Predicted rating\n",
    "                latent_prediction = 2 * np.dot(user_latent_factors[u], item_latent_factors[i])\n",
    "                c_ui[u, i] = 1 - abs(predicted_ratings[u, i] - latent_prediction)\n",
    "\n",
    "    # Normalize c_{u,i} for each item\n",
    "    c_ui_normalized = np.zeros_like(c_ui)\n",
    "    for i in range(n_items):\n",
    "        col = c_ui[:, i]\n",
    "        col_min = np.min(col)\n",
    "        col_max = np.max(col)\n",
    "\n",
    "        if col_max > col_min:  # Avoid division by zero\n",
    "            c_ui_normalized[:, i] = beta + (1 - beta) * (col - col_min) / (col_max - col_min)\n",
    "        else:  # If all values are the same, set them to beta\n",
    "            c_ui_normalized[:, i] = beta\n",
    "\n",
    "    return c_ui_normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggragating Profile use AOFRAM & W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_virtual_profile(Y, predicted_ratings, c_ui_matrix, k_ui_matrix, group = None, group_size=3):\n",
    "    \"\"\"\n",
    "    Create a virtual profile by aggregating the profiles of a group of users.\n",
    "\n",
    "    Args:\n",
    "        Y: 2D numpy array (users x items), the rating matrix with real ratings (0 for missing values).\n",
    "        predicted_ratings: 2D numpy array (users x items), predicted ratings for all users and items.\n",
    "        c_ui_matrix: Precomputed normalized c_{u,i} matrix (users x items).\n",
    "        k_ui_matrix: Precomputed normalized k_{u,i} matrix (users x items).\n",
    "        group_size: Size of the group to create.\n",
    "\n",
    "    Returns:\n",
    "        group: Index of users in the group.\n",
    "        virtual_profile: Aggregated virtual profile for the group.\n",
    "    \"\"\"\n",
    "    # Select a random group of users\n",
    "    n_users = Y.shape[0]\n",
    "    if group is None:\n",
    "            group = random.sample(range(n_users), group_size) \n",
    "    print(f\"Selected group (user indices): {group}\")\n",
    "\n",
    "    # Initialize virtual profile\n",
    "    n_items = Y.shape[1]\n",
    "    virtual_profile = np.zeros(n_items)\n",
    "\n",
    "    for item in range(n_items):\n",
    "        # Check if at least one user in the group has an observed rating\n",
    "        has_real_rating = any(Y[u, item] > 0 for u in group)\n",
    "\n",
    "        if not has_real_rating:\n",
    "            virtual_profile[item] = 0  # Leave as 0 if no observed ratings\n",
    "            continue\n",
    "\n",
    "        # Aggregate ratings for this item\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        for u in group:\n",
    "            k_ui = k_ui_matrix[u, item]\n",
    "            s_u_i = Y[u, item] if Y[u, item] > 0 else predicted_ratings[u, item]\n",
    "            c_u_i = c_ui_matrix[u, item]  # Precomputed normalized c_{u,i}\n",
    "\n",
    "            weight = k_ui * c_u_i\n",
    "            numerator += weight * s_u_i\n",
    "            denominator += weight\n",
    "\n",
    "        virtual_profile[item] = numerator / denominator if denominator > 0 else 0\n",
    "\n",
    "    return group, virtual_profile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the results\n",
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User latent factors loaded from data/output/user_latent_factors.csv\n",
      "Item latent factors loaded from data/output/item_latent_factors.csv\n",
      "User biases loaded from data/output/user_biases.csv\n",
      "Item biases loaded from data/output/item_biases.csv\n",
      "Global mean loaded from data/output/global_mean.txt\n",
      "Precision: 0.9718\n",
      "Recall: 0.9157\n",
      "F1 Score: 0.9429\n",
      "Predicted ratings saved to ./data/output/predicted_ratings.csv\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_path = '../data/ml-latest-small/ratings.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Pivot to summarise and count\n",
    "data = data.pivot(index = 'userId', columns ='movieId', values = 'rating').fillna(0)\n",
    "# data.to_csv(\"test.csv\")\n",
    "# # Split data into train and test\n",
    "\n",
    "\n",
    "# # Train the model\n",
    "mf = MFOptimized(data, K, lam, learning_rate, max_iter, print_every, tolerance)\n",
    "mf.map_ids_to_indices()\n",
    "# mf.fit()\n",
    "mf.load_latent_matrices_and_biases()\n",
    "# # # Evaluate the model\n",
    "mf.evaluate()\n",
    "mf.export_ratings(\"./data/output/predicted_ratings.csv\")\n",
    "# # print(f\"\\nOptimized MF, RMSE: {rmse:.4f}\")\n",
    "# mf.export_latent_matrices_and_biases(\"data/output\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predicted matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_u = compute_user_ratings(mf.Y,mf.n_users)\n",
    "# mf.export_ratings(\"data/output/predicted_ratings.csv\")\n",
    "mf.make_predicted_matrix()\n",
    "# x = create_virtualL_profile(mf.Y,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make c_ui matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ui_matrix = compute_normalized_c_ui(mf.Y,mf.predicted_ratings, mf.o, mf.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make k_ui matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_92150/1666605463.py:50: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  k_ui_df = pd.read_sql_query(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.66666667 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.46666667 0.55555556 0.66666667 ... 0.         0.         0.        ]\n",
      " [0.46666667 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.46666667 0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2  # Hoặc bất kỳ thư viện kết nối PostgreSQL nào\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"movie_ratings\",\n",
    "    user=\"rubyuser\",\n",
    "    password=\"rubyuser\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "\n",
    "# Truy vấn SQL\n",
    "query = \"\"\"\n",
    "WITH genre_counts AS (\n",
    "    SELECT\n",
    "        r.userid,\n",
    "        r.movieid AS item_id,\n",
    "        m.genres,\n",
    "        COUNT(r.rating) AS observed_ratings,\n",
    "        LENGTH(m.genres) - LENGTH(REPLACE(m.genres, '|', '')) + 1 AS genre_count\n",
    "    FROM\n",
    "        ratings r\n",
    "    JOIN\n",
    "        movies m ON r.movieid = m.movieid\n",
    "    WHERE\n",
    "        r.rating > 0\n",
    "    GROUP BY\n",
    "        r.userid, r.movieid, m.genres\n",
    "),\n",
    "k_ui_raw AS (\n",
    "    SELECT\n",
    "        userid,\n",
    "        item_id,\n",
    "        CAST(observed_ratings AS FLOAT) / genre_count AS k_ui\n",
    "    FROM\n",
    "        genre_counts\n",
    "),\n",
    "k_ui_normalized AS (\n",
    "    SELECT\n",
    "        userid,\n",
    "        item_id,\n",
    "        (k_ui - MIN(k_ui) OVER()) / (MAX(k_ui) OVER() - MIN(k_ui) OVER()) * (1 - 0.4) + 0.4 AS normalized_k_ui\n",
    "    FROM\n",
    "        k_ui_raw\n",
    ")\n",
    "SELECT * FROM k_ui_normalized;\n",
    "\"\"\"\n",
    "\n",
    "# Thực hiện truy vấn\n",
    "k_ui_df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Chuyển đổi dữ liệu thành ma trận numpy\n",
    "k_ui_matrix = k_ui_df.pivot(index='userid', columns='item_id', values='normalized_k_ui').fillna(0).to_numpy()\n",
    "\n",
    "# Đóng kết nối\n",
    "conn.close()\n",
    "print(k_ui_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save, or load the c_ui and k_ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to data/output/c_ui_matrix.csv\n",
      "Saved to data/output/k_ui_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "# # print(c_ui_matrix)\n",
    "output_dir = \"data/output\"\n",
    "# np.savetxt(os.path.join(output_dir, \"c_ui_matrix.csv\"), c_ui_matrix, delimiter=\",\")\n",
    "# print(f\"Saved to {os.path.join(output_dir, 'c_ui_matrix.csv')}\")\n",
    "c_ui_matrix =np.loadtxt(os.path.join(output_dir, \"c_ui_matrix.csv\"), delimiter=\",\")\n",
    "print(f\"Saved to {os.path.join(output_dir, 'c_ui_matrix.csv')}\")\n",
    "\n",
    "output_dir = \"data/output\"\n",
    "np.savetxt(os.path.join(output_dir, \"k_ui_matrix.csv\"), k_ui_matrix, delimiter=\",\")\n",
    "print(f\"Saved to {os.path.join(output_dir, 'k_ui_matrix.csv')}\")\n",
    "# np.loadtxt(os.path.join(output_dir, \"k_ui_matrix.csv\"), k_ui_matrix, delimiter=\",\")\n",
    "# print(f\"Saved to {os.path.join(output_dir, 'k_ui_matrix.csv')}\")\n",
    "# print(k_ui_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a virtual profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected group (user indices): [211, 154]\n",
      "[211, 154]\n",
      "[3. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Generate virtual profile\n",
    "group, virtual_profile = create_virtual_profile(\n",
    "    Y=mf.Y,\n",
    "    predicted_ratings=mf.predicted_ratings,\n",
    "    c_ui_matrix=c_ui_matrix,\n",
    "    k_ui_matrix=k_ui_matrix,\n",
    "    group=[211, 154],\n",
    "    group_size=3\n",
    ")\n",
    "print(group)\n",
    "print(virtual_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the profile, or load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to data/output/online/group_2/virtual_profile.csv\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"data/output/online/group_2\"\n",
    "group = [102,519,309]\n",
    "np.savetxt(os.path.join(output_dir, \"virtual_profile.csv\"), virtual_profile, delimiter=\",\")\n",
    "print(f\"Saved to {os.path.join(output_dir, 'virtual_profile.csv')}\")\n",
    "# np.loadtxt(os.path.join(output_dir, \"virtual_profile.csv\"), virtual_profile, delimiter=\",\")\n",
    "# print(f\"Saved to {os.path.join(output_dir, 'virtual_profile.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add it to the rating matrix, then give predictions for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/200, Loss: 49765.7457\n",
      "Iteration 2/200, Loss: 47218.0381\n",
      "Iteration 3/200, Loss: 45635.4283\n",
      "Iteration 4/200, Loss: 44512.5029\n",
      "Iteration 5/200, Loss: 43652.6011\n",
      "Iteration 6/200, Loss: 42959.0945\n",
      "Iteration 7/200, Loss: 42378.4149\n",
      "Iteration 8/200, Loss: 41878.5071\n",
      "Iteration 9/200, Loss: 41438.8725\n",
      "Iteration 10/200, Loss: 41045.9021\n",
      "Iteration 11/200, Loss: 40690.0213\n",
      "Iteration 12/200, Loss: 40364.3433\n",
      "Iteration 13/200, Loss: 40063.6807\n",
      "Iteration 14/200, Loss: 39784.1275\n",
      "Iteration 15/200, Loss: 39522.5233\n",
      "Iteration 16/200, Loss: 39276.4042\n",
      "Iteration 17/200, Loss: 39043.7544\n",
      "Iteration 18/200, Loss: 38822.8883\n",
      "Iteration 19/200, Loss: 38612.4320\n",
      "Iteration 20/200, Loss: 38411.1569\n",
      "Iteration 21/200, Loss: 38218.0755\n",
      "Iteration 22/200, Loss: 38032.3173\n",
      "Iteration 23/200, Loss: 37853.1196\n",
      "Iteration 24/200, Loss: 37679.7845\n",
      "Iteration 25/200, Loss: 37511.7535\n",
      "Iteration 26/200, Loss: 37348.4920\n",
      "Iteration 27/200, Loss: 37189.5553\n",
      "Iteration 28/200, Loss: 37034.4863\n",
      "Iteration 29/200, Loss: 36882.9285\n",
      "Iteration 30/200, Loss: 36734.5383\n",
      "Iteration 31/200, Loss: 36588.9639\n",
      "Iteration 32/200, Loss: 36445.9745\n",
      "Iteration 33/200, Loss: 36305.2598\n",
      "Iteration 34/200, Loss: 36166.6195\n",
      "Iteration 35/200, Loss: 36029.8155\n",
      "Iteration 36/200, Loss: 35894.6050\n",
      "Iteration 37/200, Loss: 35760.8334\n",
      "Iteration 38/200, Loss: 35628.2931\n",
      "Iteration 39/200, Loss: 35496.8396\n",
      "Iteration 40/200, Loss: 35366.3205\n",
      "Iteration 41/200, Loss: 35236.5637\n",
      "Iteration 42/200, Loss: 35107.4561\n",
      "Iteration 43/200, Loss: 34978.8420\n",
      "Iteration 44/200, Loss: 34850.6015\n",
      "Iteration 45/200, Loss: 34722.6413\n",
      "Iteration 46/200, Loss: 34594.8349\n",
      "Iteration 47/200, Loss: 34467.0961\n",
      "Iteration 48/200, Loss: 34339.3105\n",
      "Iteration 49/200, Loss: 34211.4076\n",
      "Iteration 50/200, Loss: 34083.2873\n",
      "Iteration 51/200, Loss: 33954.8811\n",
      "Iteration 52/200, Loss: 33826.0907\n",
      "Iteration 53/200, Loss: 33696.8578\n",
      "Iteration 54/200, Loss: 33567.1515\n",
      "Iteration 55/200, Loss: 33436.8714\n",
      "Iteration 56/200, Loss: 33305.9748\n",
      "Iteration 57/200, Loss: 33174.4182\n",
      "Iteration 58/200, Loss: 33042.1718\n",
      "Iteration 59/200, Loss: 32909.1792\n",
      "Iteration 60/200, Loss: 32775.3759\n",
      "Iteration 61/200, Loss: 32640.7863\n",
      "Iteration 62/200, Loss: 32505.3475\n",
      "Iteration 63/200, Loss: 32369.0456\n",
      "Iteration 64/200, Loss: 32231.8707\n",
      "Iteration 65/200, Loss: 32093.8105\n",
      "Iteration 66/200, Loss: 31954.8259\n",
      "Iteration 67/200, Loss: 31814.9655\n",
      "Iteration 68/200, Loss: 31674.2071\n",
      "Iteration 69/200, Loss: 31532.5404\n",
      "Iteration 70/200, Loss: 31389.9866\n",
      "Iteration 71/200, Loss: 31246.5715\n",
      "Iteration 72/200, Loss: 31102.2963\n",
      "Iteration 73/200, Loss: 30957.2111\n",
      "Iteration 74/200, Loss: 30811.2893\n",
      "Iteration 75/200, Loss: 30664.5894\n",
      "Iteration 76/200, Loss: 30517.1571\n",
      "Iteration 77/200, Loss: 30369.0007\n",
      "Iteration 78/200, Loss: 30220.1893\n",
      "Iteration 79/200, Loss: 30070.7496\n",
      "Iteration 80/200, Loss: 29920.7212\n",
      "Iteration 81/200, Loss: 29770.1481\n",
      "Iteration 82/200, Loss: 29619.0549\n",
      "Iteration 83/200, Loss: 29467.5630\n",
      "Iteration 84/200, Loss: 29315.6268\n",
      "Iteration 85/200, Loss: 29163.3872\n",
      "Iteration 86/200, Loss: 29010.8631\n",
      "Iteration 87/200, Loss: 28858.1184\n",
      "Iteration 88/200, Loss: 28705.1476\n",
      "Iteration 89/200, Loss: 28552.0853\n",
      "Iteration 90/200, Loss: 28398.9518\n",
      "Iteration 91/200, Loss: 28245.7953\n",
      "Iteration 92/200, Loss: 28092.6737\n",
      "Iteration 93/200, Loss: 27939.6753\n",
      "Iteration 94/200, Loss: 27786.8322\n",
      "Iteration 95/200, Loss: 27634.1598\n",
      "Iteration 96/200, Loss: 27481.7569\n",
      "Iteration 97/200, Loss: 27329.6186\n",
      "Iteration 98/200, Loss: 27177.8776\n",
      "Iteration 99/200, Loss: 27026.5285\n",
      "Iteration 100/200, Loss: 26875.6077\n",
      "Iteration 101/200, Loss: 26725.1616\n",
      "Iteration 102/200, Loss: 26575.3011\n",
      "Iteration 103/200, Loss: 26425.9729\n",
      "Iteration 104/200, Loss: 26277.2817\n",
      "Iteration 105/200, Loss: 26129.2823\n",
      "Iteration 106/200, Loss: 25981.8667\n",
      "Iteration 107/200, Loss: 25835.2716\n",
      "Iteration 108/200, Loss: 25689.3825\n",
      "Iteration 109/200, Loss: 25544.3144\n",
      "Iteration 110/200, Loss: 25400.0781\n",
      "Iteration 111/200, Loss: 25256.6559\n",
      "Iteration 112/200, Loss: 25114.1517\n",
      "Iteration 113/200, Loss: 24972.4662\n",
      "Iteration 114/200, Loss: 24831.7940\n",
      "Iteration 115/200, Loss: 24692.0376\n",
      "Iteration 116/200, Loss: 24553.2203\n",
      "Iteration 117/200, Loss: 24415.4176\n",
      "Iteration 118/200, Loss: 24278.6131\n",
      "Iteration 119/200, Loss: 24142.8877\n",
      "Iteration 120/200, Loss: 24008.1269\n",
      "Iteration 121/200, Loss: 23874.4087\n",
      "Iteration 122/200, Loss: 23741.7852\n",
      "Iteration 123/200, Loss: 23610.2363\n",
      "Iteration 124/200, Loss: 23479.7697\n",
      "Iteration 125/200, Loss: 23350.3701\n",
      "Iteration 126/200, Loss: 23222.1097\n",
      "Iteration 127/200, Loss: 23094.9090\n",
      "Iteration 128/200, Loss: 22968.9004\n",
      "Iteration 129/200, Loss: 22843.9305\n",
      "Iteration 130/200, Loss: 22720.1444\n",
      "Iteration 131/200, Loss: 22597.4741\n",
      "Iteration 132/200, Loss: 22475.8994\n",
      "Iteration 133/200, Loss: 22355.4720\n",
      "Iteration 134/200, Loss: 22236.1469\n",
      "Iteration 135/200, Loss: 22118.0306\n",
      "Iteration 136/200, Loss: 22000.9815\n",
      "Iteration 137/200, Loss: 21885.0960\n",
      "Iteration 138/200, Loss: 21770.3289\n",
      "Iteration 139/200, Loss: 21656.7142\n",
      "Iteration 140/200, Loss: 21544.1288\n",
      "Iteration 141/200, Loss: 21432.7062\n",
      "Iteration 142/200, Loss: 21322.4786\n",
      "Iteration 143/200, Loss: 21213.2955\n",
      "Iteration 144/200, Loss: 21105.2323\n",
      "Iteration 145/200, Loss: 20998.2471\n",
      "Iteration 146/200, Loss: 20892.4115\n",
      "Iteration 147/200, Loss: 20787.6534\n",
      "Iteration 148/200, Loss: 20683.8765\n",
      "Iteration 149/200, Loss: 20581.3142\n",
      "Iteration 150/200, Loss: 20479.7468\n",
      "Iteration 151/200, Loss: 20379.2404\n",
      "Iteration 152/200, Loss: 20279.8124\n",
      "Iteration 153/200, Loss: 20181.4162\n",
      "Iteration 154/200, Loss: 20084.0788\n",
      "Iteration 155/200, Loss: 19987.7439\n",
      "Iteration 156/200, Loss: 19892.4924\n",
      "Iteration 157/200, Loss: 19798.1717\n",
      "Iteration 158/200, Loss: 19704.9059\n",
      "Iteration 159/200, Loss: 19612.6629\n",
      "Iteration 160/200, Loss: 19521.3726\n",
      "Iteration 161/200, Loss: 19431.0776\n",
      "Iteration 162/200, Loss: 19341.7434\n",
      "Iteration 163/200, Loss: 19253.3258\n",
      "Iteration 164/200, Loss: 19165.8903\n",
      "Iteration 165/200, Loss: 19079.4402\n",
      "Iteration 166/200, Loss: 18993.8845\n",
      "Iteration 167/200, Loss: 18909.2643\n",
      "Iteration 168/200, Loss: 18825.5972\n",
      "Iteration 169/200, Loss: 18742.7559\n",
      "Iteration 170/200, Loss: 18660.8481\n",
      "Iteration 171/200, Loss: 18579.8434\n",
      "Iteration 172/200, Loss: 18499.6896\n",
      "Iteration 173/200, Loss: 18420.4350\n",
      "Iteration 174/200, Loss: 18342.0537\n",
      "Iteration 175/200, Loss: 18264.4270\n",
      "Iteration 176/200, Loss: 18187.7169\n",
      "Iteration 177/200, Loss: 18111.7825\n",
      "Iteration 178/200, Loss: 18036.7618\n",
      "Iteration 179/200, Loss: 17962.4956\n",
      "Iteration 180/200, Loss: 17889.0380\n",
      "Iteration 181/200, Loss: 17816.3561\n",
      "Iteration 182/200, Loss: 17744.4522\n",
      "Iteration 183/200, Loss: 17673.3436\n",
      "Iteration 184/200, Loss: 17603.0113\n",
      "Iteration 185/200, Loss: 17533.4206\n",
      "Iteration 186/200, Loss: 17464.5706\n",
      "Iteration 187/200, Loss: 17396.4702\n",
      "Iteration 188/200, Loss: 17329.0740\n",
      "Iteration 189/200, Loss: 17262.4677\n",
      "Iteration 190/200, Loss: 17196.5732\n",
      "Iteration 191/200, Loss: 17131.3180\n",
      "Iteration 192/200, Loss: 17066.7967\n",
      "Iteration 193/200, Loss: 17002.9160\n",
      "Iteration 194/200, Loss: 16939.7711\n",
      "Iteration 195/200, Loss: 16877.3207\n",
      "Iteration 196/200, Loss: 16815.4289\n",
      "Iteration 197/200, Loss: 16754.3139\n",
      "Iteration 198/200, Loss: 16693.8056\n",
      "Iteration 199/200, Loss: 16633.8763\n",
      "Iteration 200/200, Loss: 16574.6611\n"
     ]
    }
   ],
   "source": [
    "mf.Y = np.vstack([mf.Y, virtual_profile])\n",
    "mf.n_users+=1\n",
    "mf.H = np.random.normal(0, 0.1, (mf.n_users, K)).astype(np.float32)  # Latent factors for users\n",
    "mf.Q = np.random.normal(0, 0.1, (mf.n_items, K)).astype(np.float32)  # Latent factors for items)\n",
    "mf.o = np.zeros(mf.n_users, dtype=np.float32)  # Biases for users\n",
    "mf.p = np.zeros(mf.n_items, dtype=np.float32)  # Biases for items\n",
    "mf.mu = np.mean(mf.Y[mf.Y > 0])  # Global average rating (non-zero entries only)\n",
    "mf.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9730\n",
      "Recall: 0.9164\n",
      "F1 Score: 0.9439\n",
      "Predicted ratings saved to data/output/online/group_2/predicted_ratings.csv\n",
      "User latent factors saved to data/output/online/group_2/user_latent_factors.csv\n",
      "Item latent factors saved to data/output/online/group_2/item_latent_factors.csv\n",
      "User biases saved to data/output/online/group_2/user_biases.csv\n",
      "Item biases saved to data/output/online/group_2/item_biases.csv\n",
      "Global mean saved to data/output/online/group_2/global_mean.txt\n"
     ]
    }
   ],
   "source": [
    "mf.evaluate()\n",
    "mf.user_id_to_index[mf.n_users] =mf.n_users - 1 - 1\n",
    "mf.index_to_user_id[mf.n_users - 1 - 1] = mf.n_users\n",
    "\n",
    "mf.export_ratings(f\"{output_dir}/predicted_ratings.csv\")\n",
    "# # print(f\"\\nOptimized MF, RMSE: {rmse:.4f}\")\n",
    "mf.export_latent_matrices_and_biases(f\"{output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
