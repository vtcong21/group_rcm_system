{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use matrix factorization for recommender system\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import os, csv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparametters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=60 # latent factors\n",
    "lam=0.02 # regularization\n",
    "learning_rate=0.001 # learning rate\n",
    "max_iter=200 # max iterations\n",
    "print_every=1 # print loss for each iteration\n",
    "tolerance=1e-6 # tolerance\n",
    "beta=0.4 # min bound to normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create class MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MFOptimized:\n",
    "    def __init__(self, Y: pd.DataFrame, K, lam=0.1, learning_rate=0.01, max_iter=100, print_every=10, tolerance=1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the MF model.\n",
    "        Args:\n",
    "            Y (pandas array): A 2D numpy array of shape (n_users, n_items) representing the rating matrix.\n",
    "                Missing ratings should be represented as 0.\n",
    "            K (int): Number of latent factors.\n",
    "            lam (float): Regularization parameter.\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "            max_iter (int): Number of training iterations.\n",
    "            print_every (int): Print loss every print_every iterations.\n",
    "            tolerance (float): Tolerance for convergence based on change in loss.\n",
    "        \"\"\"\n",
    "        self.Y = Y\n",
    "        self.K = K\n",
    "        self.lam = lam\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.print_every = print_every\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "        # Dimensions of the rating matrix\n",
    "        self.n_users, self.n_items = Y.shape\n",
    "\n",
    "        # Initialize latent factors and biases\n",
    "        self.H = np.random.normal(0, 0.1, (self.n_users, K)).astype(np.float32)  # Latent factors for users\n",
    "        self.Q = np.random.normal(0, 0.1, (self.n_items, K)).astype(np.float32)  # Latent factors for items)\n",
    "        self.o = np.zeros(self.n_users, dtype=np.float32)  # Biases for users\n",
    "        self.p = np.zeros(self.n_items, dtype=np.float32)  # Biases for items\n",
    "        self.mu = np.mean(Y[Y > 0])  # Global average rating (non-zero entries only)\n",
    "\n",
    "    def map_ids_to_indices(self):\n",
    "        \"\"\"\n",
    "        Map original user and movie IDs to their corresponding indices in the pivoted matrix.\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame containing ratings, with userId as index and movieId as columns.\n",
    "        \n",
    "        Returns:\n",
    "            user_id_to_index: Dictionary mapping userId to row index.\n",
    "            movie_id_to_index: Dictionary mapping movieId to column index.\n",
    "            index_to_user_id: Dictionary mapping row index to userId.\n",
    "            index_to_movie_id: Dictionary mapping column index to movieId.\n",
    "        \"\"\"\n",
    "        # Create mappings for userId and movieId\n",
    "        self.user_id_to_index = {user_id: idx for idx, user_id in enumerate(self.Y.index)}\n",
    "        self.movie_id_to_index = {movie_id: idx for idx, movie_id in enumerate(self.Y.columns)}\n",
    "        \n",
    "        # Reverse mappings\n",
    "        self.index_to_user_id = {idx: user_id for user_id, idx in self.user_id_to_index.items()}\n",
    "        self.index_to_movie_id = {idx: movie_id for movie_id, idx in self.movie_id_to_index.items()}\n",
    "        self.Y = self.Y.to_numpy()\n",
    "\n",
    "    def compute_loss(self):\n",
    "        \"\"\"\n",
    "        Compute the loss based on the provided formula.\n",
    "        \"\"\"\n",
    "        mask = self.Y > 0  # Mask to filter out missing ratings\n",
    "        error_sum = 0\n",
    "        regularization_sum = 0\n",
    "\n",
    "        for u in range(self.n_users):\n",
    "            for i in range(self.n_items):\n",
    "                if mask[u, i]:\n",
    "                    r = self.Y[u, i]\n",
    "                    pred = self.o[u] + self.p[i] + self.mu + np.dot(self.H[u], self.Q[i])\n",
    "                    error = r - pred\n",
    "                    error_sum += error ** 2\n",
    "                    regularization_sum += (\n",
    "                        np.sum(self.H[u] ** 2)\n",
    "                        + np.sum(self.Q[i] ** 2)\n",
    "                        + self.o[u] ** 2\n",
    "                        + self.p[i] ** 2\n",
    "                    )\n",
    "\n",
    "        # Compute total loss\n",
    "        loss = 0.5 * error_sum + 0.5 * self.lam * regularization_sum\n",
    "        return loss\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Train the model using stochastic gradient descent (SGD).\n",
    "        \"\"\"\n",
    "        prev_loss = float('inf')\n",
    "\n",
    "        for it in range(self.max_iter):\n",
    "            for u in range(self.n_users):\n",
    "                for i in range(self.n_items):\n",
    "                    if self.Y[u, i] > 0:  # Update only for observed ratings\n",
    "                        r = self.Y[u, i]\n",
    "\n",
    "                        # Calculate prediction\n",
    "                        pred = self.o[u] + self.p[i] + self.mu + np.dot(self.H[u], self.Q[i])\n",
    "\n",
    "                        # Calculate error\n",
    "                        error = r - pred\n",
    "\n",
    "                        # Update latent factors and biases\n",
    "                        self.H[u] += self.learning_rate * (error * self.Q[i] - self.lam * self.H[u])\n",
    "                        self.Q[i] += self.learning_rate * (error * self.H[u] - self.lam * self.Q[i])\n",
    "\n",
    "                        self.o[u] += self.learning_rate * (error - self.lam * self.o[u])\n",
    "                        self.p[i] += self.learning_rate * (error - self.lam * self.p[i])\n",
    "\n",
    "            # Compute current loss and check for convergence\n",
    "            loss = self.compute_loss()\n",
    "\n",
    "            # Check if the change in loss is smaller than the tolerance\n",
    "            if abs(prev_loss - loss) < self.tolerance:\n",
    "                print(f\"Convergence reached at iteration {it + 1}\")\n",
    "                break\n",
    "\n",
    "            prev_loss = loss\n",
    "\n",
    "            # Print loss every 'print_every' iterations\n",
    "            if (it + 1) % self.print_every == 0:\n",
    "                print(f\"Iteration {it + 1}/{self.max_iter}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, u, i):\n",
    "        \"\"\"\n",
    "        Predict the rating for a specific user-item pair.\n",
    "        \"\"\"\n",
    "        u, i = int(u), int(i)\n",
    "        pred = self.o[u] + self.p[i] + self.mu + np.dot(self.H[u], self.Q[i])\n",
    "        return np.clip(pred, 0, 5)\n",
    "\n",
    "\n",
    "    def export_latent_matrices_and_biases(self, output_dir=\"data/output\"):\n",
    "        \"\"\"\n",
    "        Export the latent matrices (H, Q) and biases (o, p) to CSV files.\n",
    "        Args:\n",
    "            output_dir (str): Directory where the files will be saved.\n",
    "        \"\"\" \n",
    "        os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "        # Save user latent factors (H)\n",
    "        np.savetxt(os.path.join(output_dir, \"user_latent_factors.csv\"), self.H, delimiter=\",\")\n",
    "        print(f\"User latent factors saved to {os.path.join(output_dir, 'user_latent_factors.csv')}\")\n",
    "\n",
    "        # Save item latent factors (Q)\n",
    "        np.savetxt(os.path.join(output_dir, \"item_latent_factors.csv\"), self.Q, delimiter=\",\")\n",
    "        print(f\"Item latent factors saved to {os.path.join(output_dir, 'item_latent_factors.csv')}\")\n",
    "\n",
    "        # Save user biases (o)\n",
    "        np.savetxt(os.path.join(output_dir, \"user_biases.csv\"), self.o, delimiter=\",\")\n",
    "        print(f\"User biases saved to {os.path.join(output_dir, 'user_biases.csv')}\")\n",
    "\n",
    "        # Save item biases (p)\n",
    "        np.savetxt(os.path.join(output_dir, \"item_biases.csv\"), self.p, delimiter=\",\")\n",
    "        print(f\"Item biases saved to {os.path.join(output_dir, 'item_biases.csv')}\")\n",
    "\n",
    "        # Save global mean (mu)\n",
    "        with open(os.path.join(output_dir, \"global_mean.txt\"), \"w\") as f:\n",
    "            f.write(str(self.mu))\n",
    "        print(f\"Global mean saved to {os.path.join(output_dir, 'global_mean.txt')}\")\n",
    "\n",
    "    def load_latent_matrices_and_biases(self, input_dir=\"data/output\"):\n",
    "        \"\"\"\n",
    "        Load the latent matrices (H, Q) and biases (o, p) from CSV files.\n",
    "        Args:\n",
    "            input_dir (str): Directory where the files are saved.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load user latent factors (H)\n",
    "            self.H = np.loadtxt(os.path.join(input_dir, \"user_latent_factors.csv\"), delimiter=\",\")\n",
    "            print(f\"User latent factors loaded from {os.path.join(input_dir, 'user_latent_factors.csv')}\")\n",
    "\n",
    "            # Load item latent factors (Q)\n",
    "            self.Q = np.loadtxt(os.path.join(input_dir, \"item_latent_factors.csv\"), delimiter=\",\")\n",
    "            print(f\"Item latent factors loaded from {os.path.join(input_dir, 'item_latent_factors.csv')}\")\n",
    "\n",
    "            # Load user biases (o)\n",
    "            self.o = np.loadtxt(os.path.join(input_dir, \"user_biases.csv\"), delimiter=\",\")\n",
    "            print(f\"User biases loaded from {os.path.join(input_dir, 'user_biases.csv')}\")\n",
    "\n",
    "            # Load item biases (p)\n",
    "            self.p = np.loadtxt(os.path.join(input_dir, \"item_biases.csv\"), delimiter=\",\")\n",
    "            print(f\"Item biases loaded from {os.path.join(input_dir, 'item_biases.csv')}\")\n",
    "\n",
    "            # Load global mean (mu)\n",
    "            with open(os.path.join(input_dir, \"global_mean.txt\"), \"r\") as f:\n",
    "                self.mu = float(f.read())\n",
    "            print(f\"Global mean loaded from {os.path.join(input_dir, 'global_mean.txt')}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while loading latent matrices and biases: {e}\")\n",
    "\n",
    "    def evaluate(self, threshold=3):\n",
    "        \"\"\"\n",
    "        Evaluate the model using Precision, Recall, and F1 score.\n",
    "        Args:\n",
    "            threshold (int): The rating threshold to consider a \"positive\" prediction.\n",
    "        \"\"\"\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        # Iterate through all users and items\n",
    "        for u in range(self.n_users):\n",
    "            for i in range(self.n_items):\n",
    "                if self.Y[u, i] > 0:  # Only evaluate on observed ratings\n",
    "                    observed_rating = self.Y[u, i]\n",
    "                    predicted_rating = self.predict(u, i)\n",
    "\n",
    "                    # Convert ratings to binary (1 if positive, 0 if negative)\n",
    "                    y_true.append(1 if observed_rating >= threshold else 0)\n",
    "                    y_pred.append(1 if predicted_rating >= threshold else 0)\n",
    "\n",
    "        # Compute Precision, Recall, and F1 Score\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    def make_predicted_matrix(self):\n",
    "        self.predicted_ratings = np.zeros((self.n_users, self.n_items))\n",
    "\n",
    "        for u in range(self.n_users):\n",
    "            for i in range(self.n_items):\n",
    "                # Compute the predicted rating for each user-item pair\n",
    "                prediction = (self.o[u] + self.p[i] + self.mu +\n",
    "                            np.dot(self.H[u], self.Q[i]))\n",
    "                self.predicted_ratings[u, i] = np.clip(prediction, 0, 5)  # Clip to a valid rating range (0 to 5)\n",
    "\n",
    "    def export_ratings(self, output_filename=\"predicted_ratings.csv\"):\n",
    "        \"\"\"\n",
    "        Export the predicted and observed ratings to a CSV file, using original userId and movieId.\n",
    "        Args:\n",
    "            output_filename (str): Name of the output CSV file.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        \n",
    "        for u in range(self.n_users):\n",
    "            for i in range(self.n_items):\n",
    "                if self.Y[u, i] > 0:  # Only consider observed ratings\n",
    "                    observed_rating = self.Y[u, i]\n",
    "                    predicted_rating = self.predict(u, i)\n",
    "                    \n",
    "                    # Map the indices back to original userId and movieId\n",
    "                    original_user_id = self.index_to_user_id[u]\n",
    "                    original_movie_id = self.index_to_movie_id[i]\n",
    "                    \n",
    "                    rows.append([original_user_id, original_movie_id, predicted_rating, observed_rating])\n",
    "\n",
    "        # Create DataFrame and save to CSV\n",
    "        df = pd.DataFrame(rows, columns=[\"userId\", \"movieId\", \"predicted_ratings\", \"observed_ratings\"])\n",
    "        df.to_csv(output_filename, index=False)\n",
    "        print(f\"Predicted ratings saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $c^{[beta,1]}_{u,i}$ matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_normalized_c_ui(Y, predicted_ratings, user_latent_factors, item_latent_factors, beta=0.4):\n",
    "    \"\"\"\n",
    "    Compute and normalize c_{u,i} for the entire matrix.\n",
    "\n",
    "    Args:\n",
    "        Y (numpy.ndarray): Rating matrix (n_users x n_items), with 0 for missing ratings.\n",
    "        predicted_ratings (numpy.ndarray): Predicted ratings matrix (n_users x n_items).\n",
    "        user_latent_factors (numpy.ndarray): User latent factors matrix (n_users x K).\n",
    "        item_latent_factors (numpy.ndarray): Item latent factors matrix (n_items x K).\n",
    "        beta (float): Minimum bound for normalization (default: 0.4).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Normalized c_{u,i} matrix (n_users x n_items).\n",
    "    \"\"\"\n",
    "    n_users, n_items = Y.shape\n",
    "    c_ui = np.zeros((n_users, n_items), dtype=np.float32)\n",
    "\n",
    "    # Compute raw c_{u,i}\n",
    "    for u in range(n_users):\n",
    "        for i in range(n_items):\n",
    "            if Y[u, i] > 0:  # Observed rating\n",
    "                c_ui[u, i] = 1 - abs(Y[u, i] - predicted_ratings[u, i])\n",
    "            else:  # Predicted rating\n",
    "                latent_prediction = 2 * np.dot(user_latent_factors[u], item_latent_factors[i])\n",
    "                c_ui[u, i] = 1 - abs(predicted_ratings[u, i] - latent_prediction)\n",
    "\n",
    "    # Normalize c_{u,i} for each item\n",
    "    c_ui_normalized = np.zeros_like(c_ui)\n",
    "    for i in range(n_items):\n",
    "        col = c_ui[:, i]\n",
    "        col_min = np.min(col)\n",
    "        col_max = np.max(col)\n",
    "\n",
    "        if col_max > col_min:  # Avoid division by zero\n",
    "            c_ui_normalized[:, i] = beta + (1 - beta) * (col - col_min) / (col_max - col_min)\n",
    "        else:  # If all values are the same, set them to beta\n",
    "            c_ui_normalized[:, i] = beta\n",
    "\n",
    "    return c_ui_normalized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggragating Profile use AOFRAM & W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def create_virtual_profile(Y, predicted_ratings, c_ui_matrix, group_size=3, beta=0.4):\n",
    "    \"\"\"\n",
    "    Create a virtual profile by aggregating the profiles of a group of users.\n",
    "\n",
    "    Args:\n",
    "        Y: 2D numpy array (users x items), the rating matrix with real ratings (0 for missing values).\n",
    "        predicted_ratings: 2D numpy array (users x items), predicted ratings for all users and items.\n",
    "        user_latent_factors: 2D numpy array (users x latent factors), user latent factor matrix.\n",
    "        item_latent_factors: 2D numpy array (items x latent factors), item latent factor matrix.\n",
    "        c_ui_matrix: Precomputed normalized c_{u,i} matrix (users x items).\n",
    "        group_size: Size of the group to create.\n",
    "\n",
    "    Returns:\n",
    "        group: index of users\n",
    "        virtual_profile\n",
    "    \"\"\"\n",
    "    # Select a random group of users of the specified size\n",
    "    n_users = Y.shape[0]\n",
    "    group = random.sample(range(n_users), group_size)\n",
    "    \n",
    "    print(f\"Selected group (user indices): {group}\")\n",
    "\n",
    "    # Initialize virtual profile\n",
    "    n_items = Y.shape[1]\n",
    "    virtual_profile = np.zeros(n_items)\n",
    "\n",
    "    # Compute observed ratings and counts\n",
    "    observed_ratings_count = np.sum(Y > 0, axis=1)  # k_u for each user\n",
    "\n",
    "    # Normalize observed ratings count to the range [0.4, 1]\n",
    "    min_val, max_val = np.min(observed_ratings_count), np.max(observed_ratings_count)\n",
    "    normalized_k_u = beta + (1 - beta) * (observed_ratings_count - min_val) / (max_val - min_val)\n",
    "\n",
    "    for item in range(n_items):\n",
    "        # Check if at least one user in the group has an observed rating\n",
    "        has_real_rating = any(Y[u, item] > 0 for u in group)\n",
    "\n",
    "        if not has_real_rating:\n",
    "            # If no real ratings exist for the item, leave it as 0\n",
    "            virtual_profile[item] = 0\n",
    "            continue\n",
    "\n",
    "        # Aggregate ratings\n",
    "        numerator = 0\n",
    "        denominator = 0\n",
    "        for u in group:\n",
    "            k_u = normalized_k_u[u]\n",
    "            s_u_i = Y[u, item] if Y[u, item] > 0 else predicted_ratings[u, item]\n",
    "            c_u_i = c_ui_matrix[u, item]  # Use precomputed normalized c_{u,i}\n",
    "\n",
    "            weight = k_u * c_u_i\n",
    "            numerator += weight * s_u_i\n",
    "            denominator += weight\n",
    "\n",
    "        virtual_profile[item] = numerator / denominator if denominator > 0 else 0\n",
    "\n",
    "    # Append the virtual profile to the rating matrix\n",
    "    # Y_with_virtual = np.vstack([Y, virtual_profile])\n",
    "\n",
    "    return group, virtual_profile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the results\n",
    "Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User latent factors loaded from data/output/user_latent_factors.csv\n",
      "Item latent factors loaded from data/output/item_latent_factors.csv\n",
      "User biases loaded from data/output/user_biases.csv\n",
      "Item biases loaded from data/output/item_biases.csv\n",
      "Global mean loaded from data/output/global_mean.txt\n",
      "Precision: 0.9718\n",
      "Recall: 0.9157\n",
      "F1 Score: 0.9429\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_path = '../data/ml-latest-small/ratings.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Pivot to summarise and count\n",
    "data = data.pivot(index = 'userId', columns ='movieId', values = 'rating').fillna(0)\n",
    "# data.to_csv(\"test.csv\")\n",
    "# # Split data into train and test\n",
    "\n",
    "\n",
    "# # Train the model\n",
    "mf = MFOptimized(data, K, lam, learning_rate, max_iter, print_every, tolerance)\n",
    "mf.map_ids_to_indices()\n",
    "# mf.fit()\n",
    "mf.load_latent_matrices_and_biases()\n",
    "# # # Evaluate the model\n",
    "mf.evaluate()\n",
    "# mf.export_ratings(\"./data/output/predicted_ratings.csv\")\n",
    "# # print(f\"\\nOptimized MF, RMSE: {rmse:.4f}\")\n",
    "# mf.export_latent_matrices_and_biases(\"data/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predicted matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_u = compute_user_ratings(mf.Y,mf.n_users)\n",
    "# mf.export_ratings(\"data/output/predicted_ratings.csv\")\n",
    "mf.make_predicted_matrix()\n",
    "# x = create_virtualL_profile(mf.Y,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make c_ui matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.94420826 0.47402394 0.9671103  ... 0.4999237  0.47488588 0.45292556]\n",
      " [0.5301836  0.58072484 0.6274838  ... 0.5872026  0.5531991  0.5541628 ]\n",
      " [0.5261104  0.6782334  0.81148946 ... 0.7657037  0.63769907 0.6792286 ]\n",
      " ...\n",
      " [0.98954535 0.97414136 0.9795065  ... 0.6119779  0.59915584 0.6239989 ]\n",
      " [0.95671225 0.62834036 0.6779742  ... 0.5938318  0.6141912  0.57082635]\n",
      " [0.9342431  0.5637084  0.6238071  ... 0.58654726 0.5603328  0.54916537]]\n",
      "Selected group (user indices): [166, 405, 511]\n",
      "[[4.         0.         4.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [3.         0.         0.         ... 0.         0.         0.        ]\n",
      " [5.         0.         0.         ... 0.         0.         0.        ]\n",
      " [3.63722258 3.20148182 0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# c_ui_matrix = compute_normalized_c_ui(mf.Y,mf.predicted_ratings, mf.o, mf.p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save, or load the c_ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to data/output/c_ui_matrix.csv\n"
     ]
    }
   ],
   "source": [
    "# print(c_ui_matrix)\n",
    "output_dir = \"data/output\"\n",
    "# np.savetxt(os.path.join(output_dir, \"c_ui_matrix.csv\"), c_ui_matrix, delimiter=\",\")\n",
    "# print(f\"Saved to {os.path.join(output_dir, 'c_ui_matrix.csv')}\")\n",
    "c_ui_matrix =np.loadtxt(os.path.join(output_dir, \"c_ui_matrix.csv\"), delimiter=\",\")\n",
    "print(f\"Saved to {os.path.join(output_dir, 'c_ui_matrix.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a virtual profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected group (user indices): [211, 154]\n",
      "[211, 154]\n",
      "[3.28149398 0.         0.         ... 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "group, virtual_profile = create_virtual_profile(mf.Y, mf.predicted_ratings, c_ui_matrix, group_size=2)\n",
    "print(group)\n",
    "print(virtual_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the profile, or load it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to data/output/online/group_2/virtual_profile.csv\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"data/output/online/group_2\"\n",
    "group = [211, 154]\n",
    "np.savetxt(os.path.join(output_dir, \"virtual_profile.csv\"), virtual_profile, delimiter=\",\")\n",
    "print(f\"Saved to {os.path.join(output_dir, 'virtual_profile.csv')}\")\n",
    "# np.loadtxt(os.path.join(output_dir, \"virtual_profile.csv\"), virtual_profile, delimiter=\",\")\n",
    "# print(f\"Saved to {os.path.join(output_dir, 'virtual_profile.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add it to the rating matrix, then give predictions for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/200, Loss: 49702.9911\n",
      "Iteration 2/200, Loss: 47155.7659\n",
      "Iteration 3/200, Loss: 45573.2438\n",
      "Iteration 4/200, Loss: 44450.3956\n",
      "Iteration 5/200, Loss: 43590.6756\n",
      "Iteration 6/200, Loss: 42897.4370\n",
      "Iteration 7/200, Loss: 42317.1321\n",
      "Iteration 8/200, Loss: 41817.6374\n",
      "Iteration 9/200, Loss: 41378.5063\n",
      "Iteration 10/200, Loss: 40986.0465\n",
      "Iteration 11/200, Loss: 40630.7216\n",
      "Iteration 12/200, Loss: 40305.6170\n",
      "Iteration 13/200, Loss: 40005.5593\n",
      "Iteration 14/200, Loss: 39726.5835\n",
      "Iteration 15/200, Loss: 39465.5874\n",
      "Iteration 16/200, Loss: 39220.0700\n",
      "Iteration 17/200, Loss: 38988.0072\n",
      "Iteration 18/200, Loss: 38767.7292\n",
      "Iteration 19/200, Loss: 38557.8262\n",
      "Iteration 20/200, Loss: 38357.1210\n",
      "Iteration 21/200, Loss: 38164.6184\n",
      "Iteration 22/200, Loss: 37979.4073\n",
      "Iteration 23/200, Loss: 37800.7451\n",
      "Iteration 24/200, Loss: 37627.9577\n",
      "Iteration 25/200, Loss: 37460.4645\n",
      "Iteration 26/200, Loss: 37297.7256\n",
      "Iteration 27/200, Loss: 37139.2865\n",
      "Iteration 28/200, Loss: 36984.7302\n",
      "Iteration 29/200, Loss: 36833.6868\n",
      "Iteration 30/200, Loss: 36685.7733\n",
      "Iteration 31/200, Loss: 36540.7075\n",
      "Iteration 32/200, Loss: 36398.2099\n",
      "Iteration 33/200, Loss: 36258.0037\n",
      "Iteration 34/200, Loss: 36119.8522\n",
      "Iteration 35/200, Loss: 35983.5072\n",
      "Iteration 36/200, Loss: 35848.8225\n",
      "Iteration 37/200, Loss: 35715.5395\n",
      "Iteration 38/200, Loss: 35583.5210\n",
      "Iteration 39/200, Loss: 35452.5814\n",
      "Iteration 40/200, Loss: 35322.5735\n",
      "Iteration 41/200, Loss: 35193.3298\n",
      "Iteration 42/200, Loss: 35064.7503\n",
      "Iteration 43/200, Loss: 34936.6813\n",
      "Iteration 44/200, Loss: 34809.0029\n",
      "Iteration 45/200, Loss: 34681.6226\n",
      "Iteration 46/200, Loss: 34554.4065\n",
      "Iteration 47/200, Loss: 34427.2806\n",
      "Iteration 48/200, Loss: 34300.1150\n",
      "Iteration 49/200, Loss: 34172.8535\n",
      "Iteration 50/200, Loss: 34045.4055\n",
      "Iteration 51/200, Loss: 33917.6896\n",
      "Iteration 52/200, Loss: 33789.6264\n",
      "Iteration 53/200, Loss: 33661.1434\n",
      "Iteration 54/200, Loss: 33532.2203\n",
      "Iteration 55/200, Loss: 33402.7686\n",
      "Iteration 56/200, Loss: 33272.7064\n",
      "Iteration 57/200, Loss: 33142.0622\n",
      "Iteration 58/200, Loss: 33010.7358\n",
      "Iteration 59/200, Loss: 32878.6941\n",
      "Iteration 60/200, Loss: 32745.9138\n",
      "Iteration 61/200, Loss: 32612.3438\n",
      "Iteration 62/200, Loss: 32477.9958\n",
      "Iteration 63/200, Loss: 32342.8277\n",
      "Iteration 64/200, Loss: 32206.8396\n",
      "Iteration 65/200, Loss: 32070.0115\n",
      "Iteration 66/200, Loss: 31932.3104\n",
      "Iteration 67/200, Loss: 31793.7488\n",
      "Iteration 68/200, Loss: 31654.3610\n",
      "Iteration 69/200, Loss: 31514.0991\n",
      "Iteration 70/200, Loss: 31373.0068\n",
      "Iteration 71/200, Loss: 31231.0942\n",
      "Iteration 72/200, Loss: 31088.3487\n",
      "Iteration 73/200, Loss: 30944.8188\n",
      "Iteration 74/200, Loss: 30800.5327\n",
      "Iteration 75/200, Loss: 30655.4805\n",
      "Iteration 76/200, Loss: 30509.7476\n",
      "Iteration 77/200, Loss: 30363.2935\n",
      "Iteration 78/200, Loss: 30216.2296\n",
      "Iteration 79/200, Loss: 30068.5156\n",
      "Iteration 80/200, Loss: 29920.2519\n",
      "Iteration 81/200, Loss: 29771.4479\n",
      "Iteration 82/200, Loss: 29622.1756\n",
      "Iteration 83/200, Loss: 29472.4446\n",
      "Iteration 84/200, Loss: 29322.3371\n",
      "Iteration 85/200, Loss: 29171.8692\n",
      "Iteration 86/200, Loss: 29021.1014\n",
      "Iteration 87/200, Loss: 28870.0725\n",
      "Iteration 88/200, Loss: 28718.8388\n",
      "Iteration 89/200, Loss: 28567.4880\n",
      "Iteration 90/200, Loss: 28416.0158\n",
      "Iteration 91/200, Loss: 28264.4858\n",
      "Iteration 92/200, Loss: 28112.9894\n",
      "Iteration 93/200, Loss: 27961.5440\n",
      "Iteration 94/200, Loss: 27810.1757\n",
      "Iteration 95/200, Loss: 27658.9551\n",
      "Iteration 96/200, Loss: 27507.9295\n",
      "Iteration 97/200, Loss: 27357.1625\n",
      "Iteration 98/200, Loss: 27206.6949\n",
      "Iteration 99/200, Loss: 27056.5855\n",
      "Iteration 100/200, Loss: 26906.8388\n",
      "Iteration 101/200, Loss: 26757.4895\n",
      "Iteration 102/200, Loss: 26608.6374\n",
      "Iteration 103/200, Loss: 26460.3040\n",
      "Iteration 104/200, Loss: 26312.4930\n",
      "Iteration 105/200, Loss: 26165.3206\n",
      "Iteration 106/200, Loss: 26018.7340\n",
      "Iteration 107/200, Loss: 25872.8381\n",
      "Iteration 108/200, Loss: 25727.6044\n",
      "Iteration 109/200, Loss: 25583.1010\n",
      "Iteration 110/200, Loss: 25439.3772\n",
      "Iteration 111/200, Loss: 25296.4580\n",
      "Iteration 112/200, Loss: 25154.3062\n",
      "Iteration 113/200, Loss: 25013.0195\n",
      "Iteration 114/200, Loss: 24872.6088\n",
      "Iteration 115/200, Loss: 24733.0766\n",
      "Iteration 116/200, Loss: 24594.4665\n",
      "Iteration 117/200, Loss: 24456.7622\n",
      "Iteration 118/200, Loss: 24320.0543\n",
      "Iteration 119/200, Loss: 24184.2961\n",
      "Iteration 120/200, Loss: 24049.5413\n",
      "Iteration 121/200, Loss: 23915.8211\n",
      "Iteration 122/200, Loss: 23783.0460\n",
      "Iteration 123/200, Loss: 23651.3205\n",
      "Iteration 124/200, Loss: 23520.6856\n",
      "Iteration 125/200, Loss: 23391.0734\n",
      "Iteration 126/200, Loss: 23262.4923\n",
      "Iteration 127/200, Loss: 23135.0403\n",
      "Iteration 128/200, Loss: 23008.6129\n",
      "Iteration 129/200, Loss: 22883.3033\n",
      "Iteration 130/200, Loss: 22759.1139\n",
      "Iteration 131/200, Loss: 22635.9550\n",
      "Iteration 132/200, Loss: 22513.9602\n",
      "Iteration 133/200, Loss: 22393.0018\n",
      "Iteration 134/200, Loss: 22273.2371\n",
      "Iteration 135/200, Loss: 22154.4754\n",
      "Iteration 136/200, Loss: 22036.8578\n",
      "Iteration 137/200, Loss: 21920.2974\n",
      "Iteration 138/200, Loss: 21804.9322\n",
      "Iteration 139/200, Loss: 21690.6145\n",
      "Iteration 140/200, Loss: 21577.4077\n",
      "Iteration 141/200, Loss: 21465.3184\n",
      "Iteration 142/200, Loss: 21354.2892\n",
      "Iteration 143/200, Loss: 21244.3976\n",
      "Iteration 144/200, Loss: 21135.5975\n",
      "Iteration 145/200, Loss: 21027.8285\n",
      "Iteration 146/200, Loss: 20921.1868\n",
      "Iteration 147/200, Loss: 20815.5931\n",
      "Iteration 148/200, Loss: 20711.0624\n",
      "Iteration 149/200, Loss: 20607.5642\n",
      "Iteration 150/200, Loss: 20505.1517\n",
      "Iteration 151/200, Loss: 20403.7847\n",
      "Iteration 152/200, Loss: 20303.4851\n",
      "Iteration 153/200, Loss: 20204.2293\n",
      "Iteration 154/200, Loss: 20105.9126\n",
      "Iteration 155/200, Loss: 20008.6661\n",
      "Iteration 156/200, Loss: 19912.4263\n",
      "Iteration 157/200, Loss: 19817.2417\n",
      "Iteration 158/200, Loss: 19723.0171\n",
      "Iteration 159/200, Loss: 19629.7747\n",
      "Iteration 160/200, Loss: 19537.5208\n",
      "Iteration 161/200, Loss: 19446.2894\n",
      "Iteration 162/200, Loss: 19355.9328\n",
      "Iteration 163/200, Loss: 19266.5583\n",
      "Iteration 164/200, Loss: 19178.1896\n",
      "Iteration 165/200, Loss: 19090.6851\n",
      "Iteration 166/200, Loss: 19004.1288\n",
      "Iteration 167/200, Loss: 18918.4961\n",
      "Iteration 168/200, Loss: 18833.7855\n",
      "Iteration 169/200, Loss: 18749.9875\n",
      "Iteration 170/200, Loss: 18667.0653\n",
      "Iteration 171/200, Loss: 18585.0326\n",
      "Iteration 172/200, Loss: 18503.8765\n",
      "Iteration 173/200, Loss: 18423.5760\n",
      "Iteration 174/200, Loss: 18344.1670\n",
      "Iteration 175/200, Loss: 18265.5740\n",
      "Iteration 176/200, Loss: 18187.7736\n",
      "Iteration 177/200, Loss: 18110.8683\n",
      "Iteration 178/200, Loss: 18034.7647\n",
      "Iteration 179/200, Loss: 17959.4692\n",
      "Iteration 180/200, Loss: 17885.0678\n",
      "Iteration 181/200, Loss: 17811.3368\n",
      "Iteration 182/200, Loss: 17738.4573\n",
      "Iteration 183/200, Loss: 17666.3589\n",
      "Iteration 184/200, Loss: 17595.0637\n",
      "Iteration 185/200, Loss: 17524.4268\n",
      "Iteration 186/200, Loss: 17454.6273\n",
      "Iteration 187/200, Loss: 17385.5105\n",
      "Iteration 188/200, Loss: 17317.1925\n",
      "Iteration 189/200, Loss: 17249.5951\n",
      "Iteration 190/200, Loss: 17182.7123\n",
      "Iteration 191/200, Loss: 17116.4811\n",
      "Iteration 192/200, Loss: 17051.0355\n",
      "Iteration 193/200, Loss: 16986.1925\n",
      "Iteration 194/200, Loss: 16922.1740\n",
      "Iteration 195/200, Loss: 16858.7405\n",
      "Iteration 196/200, Loss: 16795.9449\n",
      "Iteration 197/200, Loss: 16733.8462\n",
      "Iteration 198/200, Loss: 16672.4680\n",
      "Iteration 199/200, Loss: 16611.7097\n",
      "Iteration 200/200, Loss: 16551.5689\n"
     ]
    }
   ],
   "source": [
    "mf.Y = np.vstack([mf.Y, virtual_profile])\n",
    "mf.n_users+=1\n",
    "mf.H = np.random.normal(0, 0.1, (mf.n_users, K)).astype(np.float32)  # Latent factors for users\n",
    "mf.Q = np.random.normal(0, 0.1, (mf.n_items, K)).astype(np.float32)  # Latent factors for items)\n",
    "mf.o = np.zeros(mf.n_users, dtype=np.float32)  # Biases for users\n",
    "mf.p = np.zeros(mf.n_items, dtype=np.float32)  # Biases for items\n",
    "mf.mu = np.mean(mf.Y[mf.Y > 0])  # Global average rating (non-zero entries only)\n",
    "mf.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9717\n",
      "Recall: 0.9161\n",
      "F1 Score: 0.9431\n",
      "Predicted ratings saved to data/output/online/group_2/predicted_ratings.csv\n",
      "User latent factors saved to data/output/online/group_2/user_latent_factors.csv\n",
      "Item latent factors saved to data/output/online/group_2/item_latent_factors.csv\n",
      "User biases saved to data/output/online/group_2/user_biases.csv\n",
      "Item biases saved to data/output/online/group_2/item_biases.csv\n",
      "Global mean saved to data/output/online/group_2/global_mean.txt\n"
     ]
    }
   ],
   "source": [
    "mf.evaluate()\n",
    "mf.user_id_to_index[mf.n_users] =mf.n_users - 1\n",
    "mf.index_to_user_id[mf.n_users - 1] = mf.n_users\n",
    "mf.export_ratings(f\"{output_dir}/predicted_ratings.csv\")\n",
    "# # print(f\"\\nOptimized MF, RMSE: {rmse:.4f}\")\n",
    "mf.export_latent_matrices_and_biases(f\"{output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
