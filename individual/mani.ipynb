{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use matrix factorization for recommender system\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create class MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class MFOptimized(object):\n",
    "    def __init__(self, Y, K, lam=0.1, learning_rate=0.01, max_iter=100, print_every=10, batch_size=1000):\n",
    "        self.Y = Y  # rating data: user_id, item_id, rating\n",
    "        self.K = K  # latent dimension\n",
    "        self.lam = lam  # regularization parameter\n",
    "        self.learning_rate = learning_rate  # learning rate\n",
    "        self.max_iter = max_iter  # number of iterations\n",
    "        self.print_every = print_every  # print loss every few iterations\n",
    "        self.batch_size = batch_size  # batch size for processing\n",
    "        \n",
    "        # Initialize dimensions\n",
    "        self.n_users = int(np.max(Y[:, 0]) + 1)\n",
    "        self.n_items = int(np.max(Y[:, 1]) + 1)\n",
    "        \n",
    "        # Initialize parameters (float32 for memory efficiency)\n",
    "        self.X = np.random.randn(self.n_items, K).astype(np.float32)  # item latent factors\n",
    "        self.W = np.random.randn(K, self.n_users).astype(np.float32)  # user latent factors\n",
    "        self.b = np.zeros(self.n_items, dtype=np.float32)  # item bias\n",
    "        self.d = np.zeros(self.n_users, dtype=np.float32)  # user bias\n",
    "    \n",
    "    def loss(self):\n",
    "        \"\"\"\n",
    "        Compute the loss function with regularization\n",
    "        \"\"\"\n",
    "        n_ratings = self.Y.shape[0]\n",
    "        indices = np.random.choice(n_ratings, self.batch_size, replace=False)\n",
    "        batch = self.Y[indices]\n",
    "        \n",
    "        user_ids = batch[:, 0].astype(int)\n",
    "        item_ids = batch[:, 1].astype(int)\n",
    "        ratings = batch[:, 2]\n",
    "        \n",
    "        pred = self.X[item_ids].dot(self.W[:, user_ids]) + self.b[item_ids][:, None] + self.d[user_ids]\n",
    "        error = pred - ratings[:, None]\n",
    "        loss = 0.5 * np.mean(error ** 2)\n",
    "        \n",
    "        # Add regularization\n",
    "        loss += 0.5 * self.lam * (np.sum(self.X ** 2) + np.sum(self.W ** 2))\n",
    "        return loss\n",
    "    \n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Train the MF model using mini-batch SGD\n",
    "        \"\"\"\n",
    "        for it in range(self.max_iter):\n",
    "            n_ratings = self.Y.shape[0]\n",
    "            indices = np.arange(n_ratings)\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for i in range(0, n_ratings, self.batch_size):\n",
    "                batch_indices = indices[i:i + self.batch_size]\n",
    "                batch = self.Y[batch_indices]\n",
    "                \n",
    "                user_ids = batch[:, 0].astype(int)\n",
    "                item_ids = batch[:, 1].astype(int)\n",
    "                ratings = batch[:, 2]\n",
    "                \n",
    "                # Compute predictions\n",
    "                pred = self.X[item_ids].dot(self.W[:, user_ids]) + self.b[item_ids][:, None] + self.d[user_ids]\n",
    "                error = pred - ratings[:, None]\n",
    "                \n",
    "                # Gradients\n",
    "                grad_X = (error.dot(self.W[:, user_ids].T) + self.lam * self.X[item_ids]) / self.batch_size\n",
    "                grad_W = (self.X[item_ids].T.dot(error) + self.lam * self.W[:, user_ids]) / self.batch_size\n",
    "                grad_b = np.sum(error, axis=1) / self.batch_size\n",
    "                grad_d = np.sum(error, axis=0) / self.batch_size\n",
    "                \n",
    "                # Updates\n",
    "                np.add.at(self.X, item_ids, -self.learning_rate * grad_X)\n",
    "                np.add.at(self.W.T, user_ids, -self.learning_rate * grad_W.T)\n",
    "                np.add.at(self.b, item_ids, -self.learning_rate * grad_b)\n",
    "                np.add.at(self.d, user_ids, -self.learning_rate * grad_d)\n",
    "            \n",
    "            if (it + 1) % self.print_every == 0:\n",
    "                print(f\"Iteration {it + 1}/{self.max_iter}, Loss: {self.loss():.4f}\")\n",
    "    \n",
    "    def pred(self, u, i):\n",
    "        \"\"\"\n",
    "        Predict the rating of user u for item i\n",
    "        \"\"\"\n",
    "        u, i = int(u), int(i)\n",
    "        pred = self.X[i].dot(self.W[:, u]) + self.b[i] + self.d[u]\n",
    "        return np.clip(pred, 0, 5)\n",
    "    \n",
    "    def evaluate_RMSE(self, rate_test):\n",
    "        \"\"\"\n",
    "        Evaluate RMSE on the test set\n",
    "        \"\"\"\n",
    "        n_tests = rate_test.shape[0]\n",
    "        SE = 0\n",
    "        for n in range(n_tests):\n",
    "            pred = self.pred(rate_test[n, 0], rate_test[n, 1])\n",
    "            SE += (pred - rate_test[n, 2]) ** 2\n",
    "        RMSE = np.sqrt(SE / n_tests)\n",
    "        return RMSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 5/30, Loss: 48272.0219\n",
      "Iteration 10/30, Loss: 48254.8453\n",
      "Iteration 15/30, Loss: 48250.8975\n",
      "Iteration 20/30, Loss: 48249.6982\n",
      "Iteration 25/30, Loss: 48249.2427\n",
      "Iteration 30/30, Loss: 48249.0606\n",
      "\n",
      "Optimized MF, RMSE: 0.9666\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_path = '../data/ml-latest-small/ratings.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data = data.rename(columns={'userId': 'user_id', 'movieId': 'movie_id'})\n",
    "data = data[['user_id', 'movie_id', 'rating']]\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "rate_train = train_data.to_numpy()\n",
    "rate_test = test_data.to_numpy()\n",
    "\n",
    "# Adjust indices\n",
    "rate_train[:, :2] -= 1\n",
    "rate_test[:, :2] -= 1\n",
    "\n",
    "# Train model\n",
    "mf = MFOptimized(rate_train, K=50, lam=0.01, learning_rate=0.01, max_iter=30, print_every=5)\n",
    "mf.fit()\n",
    "\n",
    "# Evaluate model\n",
    "rmse = mf.evaluate_RMSE(rate_test)\n",
    "print(f\"\\nOptimized MF, RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to interrupt the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.8.10' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dự đoán rating cho tất cả các cặp (user, movie)\n",
    "def generate_predictions(model, n_users, n_items):\n",
    "    predictions = []\n",
    "    for user in range(n_users):\n",
    "        for item in range(n_items):\n",
    "            pred_rating = model.pred(user, item)\n",
    "            predictions.append((user + 1, item + 1, pred_rating))  # Chuyển index về từ 1\n",
    "    return predictions\n",
    "\n",
    "# Tạo dự đoán\n",
    "n_users = mf.n_users\n",
    "n_items = mf.n_items\n",
    "predictions = generate_predictions(mf, n_users, n_items)\n",
    "\n",
    "# Chuyển dự đoán thành DataFrame\n",
    "predictions_df = pd.DataFrame(predictions, columns=['user_id', 'movie_id', 'predicted_rating'])\n",
    "\n",
    "# Xuất ra file CSV\n",
    "output_path = \"predicted_ratings.csv\"\n",
    "predictions_df.to_csv(output_path, index=False)\n",
    "print(f\"Predicted ratings saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
