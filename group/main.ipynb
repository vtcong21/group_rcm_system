{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use matrix factorization for recommender system\n",
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import os, csv\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparametters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=60 # latent factors\n",
    "lam=0.02 # regularization\n",
    "learning_rate=0.001 # learning rate\n",
    "max_iter=5 # max iterations\n",
    "print_every=1 # print loss for each iteration\n",
    "tolerance=1e-6 # tolerance\n",
    "beta=0.4 # min bound to normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create class MF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class MFOptimized:\n",
    "    def __init__(self, Y, K, lam=0.1, learning_rate=0.01, max_iter=100, print_every=10, tolerance=1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the MF model.\n",
    "        Args:\n",
    "            Y (numpy array): A 2D numpy array of shape (n_users, n_items) representing the rating matrix.\n",
    "                Missing ratings should be represented as 0.\n",
    "            K (int): Number of latent factors.\n",
    "            lam (float): Regularization parameter.\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "            max_iter (int): Number of training iterations.\n",
    "            print_every (int): Print loss every print_every iterations.\n",
    "            tolerance (float): Tolerance for convergence based on change in loss.\n",
    "        \"\"\"\n",
    "        self.Y = Y\n",
    "        self.K = K\n",
    "        self.lam = lam\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.print_every = print_every\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "        # Dimensions of the rating matrix\n",
    "        self.n_users, self.n_items = Y.shape\n",
    "\n",
    "        # Initialize latent factors and biases\n",
    "        self.H = np.random.normal(0, 0.1, (self.n_users, K)).astype(np.float32)  # Latent factors for users\n",
    "        self.Q = np.random.normal(0, 0.1, (self.n_items, K)).astype(np.float32)  # Latent factors for items\n",
    "        print(self.H.shape)\n",
    "        print(self.Q.shape)\n",
    "        self.o = np.zeros(self.n_users, dtype=np.float32)  # Biases for users\n",
    "        self.p = np.zeros(self.n_items, dtype=np.float32)  # Biases for items\n",
    "        self.mu = np.mean(Y[Y > 0])  # Global average rating (non-zero entries only)\n",
    "\n",
    "    def compute_loss(self):\n",
    "        \"\"\"\n",
    "        Compute the loss based on the provided formula.\n",
    "        \"\"\"\n",
    "        mask = self.Y > 0  # Mask to filter out missing ratings\n",
    "        error_sum = 0\n",
    "        regularization_sum = 0\n",
    "\n",
    "        for u in range(self.n_users):\n",
    "            for i in range(self.n_items):\n",
    "                if mask[u, i]:\n",
    "                    r = self.Y[u, i]\n",
    "                    pred = self.o[u] + self.p[i] + self.mu + np.dot(self.H[u], self.Q[i])\n",
    "                    error = r - pred\n",
    "                    error_sum += error ** 2\n",
    "                    regularization_sum += (\n",
    "                        np.sum(self.H[u] ** 2)\n",
    "                        + np.sum(self.Q[i] ** 2)\n",
    "                        + self.o[u] ** 2\n",
    "                        + self.p[i] ** 2\n",
    "                    )\n",
    "\n",
    "        # Compute total loss\n",
    "        loss = 0.5 * error_sum + 0.5 * self.lam * regularization_sum\n",
    "        return loss\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Train the model using stochastic gradient descent (SGD).\n",
    "        \"\"\"\n",
    "        prev_loss = float('inf')\n",
    "\n",
    "        for it in range(self.max_iter):\n",
    "            for u in range(self.n_users):\n",
    "                for i in range(self.n_items):\n",
    "                    if self.Y[u, i] > 0:  # Update only for observed ratings\n",
    "                        r = self.Y[u, i]\n",
    "\n",
    "                        # Calculate prediction\n",
    "                        pred = self.o[u] + self.p[i] + self.mu + np.dot(self.H[u], self.Q[i])\n",
    "\n",
    "                        # Calculate error\n",
    "                        error = r - pred\n",
    "\n",
    "                        # Update latent factors and biases\n",
    "                        self.H[u] += self.learning_rate * (error * self.Q[i] - self.lam * self.H[u])\n",
    "                        self.Q[i] += self.learning_rate * (error * self.H[u] - self.lam * self.Q[i])\n",
    "\n",
    "                        self.o[u] += self.learning_rate * (error - self.lam * self.o[u])\n",
    "                        self.p[i] += self.learning_rate * (error - self.lam * self.p[i])\n",
    "\n",
    "            # Compute current loss and check for convergence\n",
    "            loss = self.compute_loss()\n",
    "\n",
    "            # Check if the change in loss is smaller than the tolerance\n",
    "            if abs(prev_loss - loss) < self.tolerance:\n",
    "                print(f\"Convergence reached at iteration {it + 1}\")\n",
    "                break\n",
    "\n",
    "            prev_loss = loss\n",
    "\n",
    "            # Print loss every 'print_every' iterations\n",
    "            if (it + 1) % self.print_every == 0:\n",
    "                print(f\"Iteration {it + 1}/{self.max_iter}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, u, i):\n",
    "        \"\"\"\n",
    "        Predict the rating for a specific user-item pair.\n",
    "        \"\"\"\n",
    "        u, i = int(u), int(i)\n",
    "        pred = self.o[u] + self.p[i] + self.mu + np.dot(self.H[u], self.Q[i])\n",
    "        return np.clip(pred, 0, 5)\n",
    "\n",
    "\n",
    "    def export_latent_matrices_and_biases(self, output_dir=\"data/output\"):\n",
    "        \"\"\"\n",
    "        Export the latent matrices (H, Q) and biases (o, p) to CSV files.\n",
    "        Args:\n",
    "            output_dir (str): Directory where the files will be saved.\n",
    "        \"\"\" \n",
    "        os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it doesn't exist\n",
    "\n",
    "        # Save user latent factors (H)\n",
    "        np.savetxt(os.path.join(output_dir, \"user_latent_factors.csv\"), self.H, delimiter=\",\")\n",
    "        print(f\"User latent factors saved to {os.path.join(output_dir, 'user_latent_factors.csv')}\")\n",
    "\n",
    "        # Save item latent factors (Q)\n",
    "        np.savetxt(os.path.join(output_dir, \"item_latent_factors.csv\"), self.Q, delimiter=\",\")\n",
    "        print(f\"Item latent factors saved to {os.path.join(output_dir, 'item_latent_factors.csv')}\")\n",
    "\n",
    "        # Save user biases (o)\n",
    "        np.savetxt(os.path.join(output_dir, \"user_biases.csv\"), self.o, delimiter=\",\")\n",
    "        print(f\"User biases saved to {os.path.join(output_dir, 'user_biases.csv')}\")\n",
    "\n",
    "        # Save item biases (p)\n",
    "        np.savetxt(os.path.join(output_dir, \"item_biases.csv\"), self.p, delimiter=\",\")\n",
    "        print(f\"Item biases saved to {os.path.join(output_dir, 'item_biases.csv')}\")\n",
    "\n",
    "        # Save global mean (mu)\n",
    "        with open(os.path.join(output_dir, \"global_mean.txt\"), \"w\") as f:\n",
    "            f.write(str(self.mu))\n",
    "        print(f\"Global mean saved to {os.path.join(output_dir, 'global_mean.txt')}\")\n",
    "\n",
    "    def evaluate(self, threshold=3):\n",
    "        \"\"\"\n",
    "        Evaluate the model using Precision, Recall, and F1 score.\n",
    "        Args:\n",
    "            threshold (int): The rating threshold to consider a \"positive\" prediction.\n",
    "        \"\"\"\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        # Iterate through all users and items\n",
    "        for u in range(self.n_users):\n",
    "            for i in range(self.n_items):\n",
    "                if self.Y[u, i] > 0:  # Only evaluate on observed ratings\n",
    "                    observed_rating = self.Y[u, i]\n",
    "                    predicted_rating = self.predict(u, i)\n",
    "\n",
    "                    # Convert ratings to binary (1 if positive, 0 if negative)\n",
    "                    y_true.append(1 if observed_rating >= threshold else 0)\n",
    "                    y_pred.append(1 if predicted_rating >= threshold else 0)\n",
    "\n",
    "        # Compute Precision, Recall, and F1 Score\n",
    "        precision = precision_score(y_true, y_pred)\n",
    "        recall = recall_score(y_true, y_pred)\n",
    "        f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    def export_ratings(self, output_filename=\"predicted_ratings.csv\"):\n",
    "        \"\"\"\n",
    "        Export the predicted and observed ratings to a CSV file.\n",
    "        Args:\n",
    "            output_filename (str): Name of the output CSV file.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        \n",
    "        for u in range(self.n_users):\n",
    "            for i in range(self.n_items):\n",
    "                if self.Y[u, i] > 0:  # Only consider observed ratings\n",
    "                    observed_rating = self.Y[u, i]\n",
    "                    predicted_rating = self.predict(u, i)\n",
    "                    rows.append([u, i, predicted_rating, observed_rating])\n",
    "\n",
    "        # Create DataFrame and save to CSV\n",
    "        df = pd.DataFrame(rows, columns=[\"userId\", \"movieId\", \"predicted_ratings\", \"observed_ratings\"])\n",
    "        df.to_csv(output_filename, index=False)\n",
    "        print(f\"Predicted ratings saved to {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing step\n",
    "Compute k_u: number of observed ratings of each users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_user_ratings(Y, n_users):\n",
    "    \"\"\"\n",
    "    Compute the number of observed ratings for each user.\n",
    "    Args:\n",
    "        Y: numpy array, shape (n_users, n_items), each element is the rating for user u and item i.\n",
    "        n_users: Total number of users.\n",
    "    Returns:\n",
    "        k_u: numpy array, number of ratings for each user.\n",
    "    \"\"\"\n",
    "    k_u = np.zeros(n_users, dtype=int)\n",
    "    \n",
    "    # Tính số lượng ratings cho mỗi user, chỉ tính những giá trị không phải 0\n",
    "    for u in range(n_users):\n",
    "        k_u[u] = np.sum(Y[u, :] > 0)  # Đếm số lượng rating (không phải 0) cho mỗi user\n",
    "    \n",
    "    return k_u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate groups: randomly create groups of 2/3/4 members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unique_groups_from_matrix(Y, group_size, n_groups=1000):\n",
    "    \"\"\"\n",
    "    Generate unique groups of users with a given size from a rating matrix.\n",
    "    Args:\n",
    "        Y: numpy array, shape (n_users, n_items), the rating matrix.\n",
    "        group_size: Size of each group to generate.\n",
    "        n_groups: Number of groups to generate.\n",
    "    Returns:\n",
    "        groups: List of unique groups, where each group is a tuple of user indices.\n",
    "    \"\"\"\n",
    "    n_users = Y.shape[0]\n",
    "    groups = set()  # Use a set to ensure uniqueness\n",
    "    \n",
    "    while len(groups) < n_groups:\n",
    "        # Generate a random group of the specified size\n",
    "        group = tuple(sorted(np.random.choice(n_users, group_size, replace=False)))\n",
    "        \n",
    "        # Ensure the group is unique based on their ratings (only include active users)\n",
    "        valid_group = True\n",
    "        for user in group:\n",
    "            # You can apply any condition here, for example, ensure users have at least one rating\n",
    "            if np.sum(Y[user, :] > 0) == 0:  # Check if the user has at least one rating\n",
    "                valid_group = False\n",
    "                break\n",
    "        \n",
    "        if valid_group:\n",
    "            groups.add(group)\n",
    "    \n",
    "    # Convert the set to a list for consistency in output format\n",
    "    return list(groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline\n",
    "Compute $c^{[\\beta,1]}_{u,i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cui_beta(u, i, real_ratings, predicted_ratings, mu, user_biases, item_biases, H, Q, beta=0.4):\n",
    "    \"\"\"\n",
    "    Compute the weight c_{u,i} for user u and item i, with normalization.\n",
    "    \n",
    "    Args:\n",
    "        u: User index.\n",
    "        i: Item index.\n",
    "        real_ratings: 2D array (user, item) for real ratings (NaN if missing).\n",
    "        predicted_ratings: 2D array (user, item) for predicted ratings.\n",
    "        mu: Global average rating.\n",
    "        user_biases: Array of user biases.\n",
    "        item_biases: Array of item biases.\n",
    "        H: User latent factors (n_users x K).\n",
    "        Q: Item latent factors (n_items x K).\n",
    "        beta: The lower bound for normalization.\n",
    "    \n",
    "    Returns:\n",
    "        c_u_i: Normalized weight [beta, 1].\n",
    "    \"\"\"\n",
    "    # Get the real rating (if available) and predicted rating\n",
    "    r_ui = real_ratings[u, i] if not np.isnan(real_ratings[u, i]) else None\n",
    "    pred_ui = predicted_ratings[u, i]\n",
    "    \n",
    "    # Calculate the absolute difference based on the real rating vs predicted rating\n",
    "    if r_ui is not None:  # If the rating is available\n",
    "        abs_diff = abs(r_ui - pred_ui)\n",
    "    else:  # If the rating is missing (use predicted rating)\n",
    "        abs_diff = abs(mu + user_biases[u] + item_biases[i] - np.dot(H[u], Q[i]))\n",
    "    \n",
    "    # Normalize the absolute difference to [0, 1]\n",
    "    c_u_i = 1 - abs_diff\n",
    "    c_u_i = np.clip(c_u_i, 0, 1)  # Ensure it's within [0, 1]\n",
    "    \n",
    "    # Normalization to the range [beta, 1] for the group\n",
    "    min_c = np.min(c_u_i)  # Replace with the minimum value of c_u_i in the group (G)\n",
    "    max_c = np.max(c_u_i)  # Replace with the maximum value of c_u_i in the group (G)\n",
    "    \n",
    "    c_u_i_normalized = beta + (1 - beta) * (c_u_i - min_c) / (max_c - min_c)\n",
    "    c_u_i_normalized = np.clip(c_u_i_normalized, beta, 1)  # Ensure it's within [beta, 1]\n",
    "    \n",
    "    return c_u_i_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $k^{[\\beta,1]}_{u}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_k_u_beta(k_u, beta=0.4):\n",
    "    \"\"\"\n",
    "    Compute the number of actual ratings for each users with normalization.\n",
    "    Args:\n",
    "        k_u: Array of number of ratings per user.\n",
    "        beta: Minimum bound for \\( k_u \\) normalization (default 0.4).\n",
    "    Returns:\n",
    "         k_u: Array of number of ratings per user normalized.\n",
    "    \"\"\"\n",
    "    k_min = np.min(k_u)\n",
    "    k_max = np.max(k_u)\n",
    "    k_u_normalized = beta + (1 - beta) * (k_u - k_min) / (k_max - k_min) if k_max > k_min else np.full_like(k_u, beta)\n",
    "    return k_u_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggragating Profile use AOFRAM & W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_group_profile(group, item, real_ratings, predicted_ratings, H, Q, k_u_normalized, c_ui_normalized, beta=0.4):\n",
    "    \"\"\"\n",
    "    Compute the aggregated group profile for a given item using normalization.\n",
    "\n",
    "    Args:\n",
    "        group: List of user IDs in the group.\n",
    "        item: Target item ID.\n",
    "        real_ratings: 2D array (user, item) for real ratings (nan if missing).\n",
    "        predicted_ratings: 2D array (user, item) for predicted ratings.\n",
    "        H: User latent factors (n_users x K).\n",
    "        Q: Item latent factors (n_items x K).\n",
    "        k_u_normalized: Array of number of ratings per user.\n",
    "        c_ui_normalized: Normalized weight [beta, 1].\n",
    "        beta: Minimum bound for \\( k_u \\) normalization (default 0.4).\n",
    "\n",
    "    Returns:\n",
    "        r_v_i: Aggregated group profile rating for the item (or None if no real ratings exist).\n",
    "    \"\"\"\n",
    "    # Identify real ratings for the group on the target item\n",
    "    real_group_ratings = [real_ratings[u, item] for u in group]\n",
    "    has_real_rating = any(not np.isnan(r) for r in real_group_ratings)\n",
    "\n",
    "    # If no real rating exists, return None\n",
    "    if not has_real_rating:\n",
    "        return None\n",
    "\n",
    "    # Compute weighted sums\n",
    "    for idx, u in enumerate(group):\n",
    "        c_u_i = c_ui_normalized[idx]\n",
    "        k_u_component = k_u_normalized[u]\n",
    "\n",
    "        if not np.isnan(real_ratings[u, item]):  # Real rating exists\n",
    "            s_u_i = real_ratings[u, item]\n",
    "        else:  # Use predicted rating\n",
    "            s_u_i = predicted_ratings[u, item]\n",
    "\n",
    "        # Compute weight\n",
    "        weight = k_u_component * (c_u_i ** 0.4)\n",
    "\n",
    "        # Accumulate weighted sums\n",
    "        numerator += weight * s_u_i\n",
    "        denominator += weight\n",
    "\n",
    "    # Compute aggregated profile rating\n",
    "    r_v_i = numerator / denominator if denominator > 0 else None\n",
    "    return r_v_i\n",
    "\n",
    "\n",
    "def save_group_profiles(groups, real_ratings, predicted_ratings, user_biases, item_biases, mu, H, Q, k_u, output_file=\"group_profiles.csv\"):\n",
    "    \"\"\"\n",
    "    Generate and save group profiles into a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        groups: Dictionary containing groups for different sizes.\n",
    "        real_ratings: 2D array (user, item) for real ratings (NaN if missing).\n",
    "        predicted_ratings: 2D array (user, item) for predicted ratings.\n",
    "        user_biases: Array of user biases.\n",
    "        item_biases: Array of item biases.\n",
    "        mu: Global average rating.\n",
    "        H: User latent factors (n_users x K).\n",
    "        Q: Item latent factors (n_items x K).\n",
    "        k_u: Array of number of ratings per user.\n",
    "        output_file: Path where to save the group profiles CSV.\n",
    "    \"\"\"\n",
    "    group_profiles = []\n",
    "\n",
    "    # Loop through each group size\n",
    "    for size, group_list in groups.items():\n",
    "        for group_id, group in enumerate(group_list):\n",
    "            # Loop through each item\n",
    "            for item in range(real_ratings.shape[1]):\n",
    "                # Compute the group profile rating for this group and item\n",
    "                r_v_i = compute_group_profile(group, item, real_ratings, predicted_ratings, user_biases, item_biases, mu, H, Q, k_u)\n",
    "                \n",
    "                # If we computed a valid profile, add it to the list\n",
    "                if r_v_i is not None:\n",
    "                    group_profiles.append([group_id, group, item, r_v_i])\n",
    "    \n",
    "    # Save the group profiles to a CSV file\n",
    "    with open(output_file, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        # Write header\n",
    "        writer.writerow([\"Group_ID\", \"Group_Members\", \"Item_ID\", \"Aggregated_Rating\"])\n",
    "        \n",
    "        # Write the group profiles\n",
    "        for profile in group_profiles:\n",
    "            writer.writerow(profile)\n",
    "\n",
    "    print(f\"Group profiles saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the results\n",
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(457, 60)\n",
      "(9724, 60)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data_path = '../data/ml-latest-small/ratings.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Pivot to summarise and count\n",
    "data = data.pivot(index = 'userId', columns ='movieId', values = 'rating').fillna(0)\n",
    "\n",
    "# Split data into train and test\n",
    "train_data, test_data = train_test_split(data, test_size=None, random_state=42)\n",
    "rate_train = train_data.to_numpy()\n",
    "rate_test = test_data.to_numpy()\n",
    "\n",
    "\n",
    "# Train the model\n",
    "mf = MFOptimized(rate_train, K, lam, learning_rate, max_iter, print_every, tolerance)\n",
    "# # mf.load(\"data/input\")\n",
    "# mf.fit()\n",
    "# # Evaluate the model\n",
    "# mf.evaluate()\n",
    "# mf.export_ratings(\"./data/output/predicted_ratings.csv\")\n",
    "# # print(f\"\\nOptimized MF, RMSE: {rmse:.4f}\")\n",
    "# mf.export_latent_matrices_and_biases(\"data/outut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(610, 9724)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a prediction for a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_for_user(model, user_id, n_items):\n",
    "    predictions = [(user_id + 1, item + 1, model.predict(user_id, item)) for item in range(n_items)]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make comparision of real ratings and model's predicted ratings for a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_csv(model, user_id, data, n_items, output_path):\n",
    "    user_ratings = data[data['user_id'] == user_id + 1][['user_id', 'movie_id', 'rating']]\n",
    "    predictions = generate_predictions_for_user(model, user_id, n_items)\n",
    "    predictions_df = pd.DataFrame(predictions, columns=['user_id', 'movie_id', 'predicted_rating'])\n",
    "    predictions_df['predicted_rating'] = predictions_df['predicted_rating'].round(2)\n",
    "    comparison_df = pd.merge(user_ratings, predictions_df, on=['user_id', 'movie_id'])\n",
    "    comparison_df.to_csv(output_path, index=False)\n",
    "    print(f\"Comparison CSV saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 193609 is out of bounds for axis 0 with size 193609",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m user_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m      3\u001b[0m n_items \u001b[38;5;241m=\u001b[39m mf\u001b[38;5;241m.\u001b[39mn_items\n\u001b[0;32m----> 4\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_predictions_for_user\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_items\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m predictions_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(predictions, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovie_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_rating\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      6\u001b[0m predictions_output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/output/predicted_ratings_user_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m, in \u001b[0;36mgenerate_predictions_for_user\u001b[0;34m(model, user_id, n_items)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_predictions_for_user\u001b[39m(model, user_id, n_items):\n\u001b[0;32m----> 2\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m [(user_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, item \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, model\u001b[38;5;241m.\u001b[39mpredict(user_id, item)) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_items)]\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_predictions_for_user\u001b[39m(model, user_id, n_items):\n\u001b[0;32m----> 2\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m [(user_id \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, item \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_items)]\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "Cell \u001b[0;32mIn[20], line 95\u001b[0m, in \u001b[0;36mMFOptimized.predict\u001b[0;34m(self, u, i)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03mPredict the rating for a specific user-item pair.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m u, i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(u), \u001b[38;5;28mint\u001b[39m(i)\n\u001b[0;32m---> 95\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo[u] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmu \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mH[u], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ[i])\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mclip(pred, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 193609 is out of bounds for axis 0 with size 193609"
     ]
    }
   ],
   "source": [
    "user_id = 100\n",
    "\n",
    "n_items = mf.n_items\n",
    "predictions = generate_predictions_for_user(mf, user_id, n_items)\n",
    "predictions_df = pd.DataFrame(predictions, columns=['user_id', 'movie_id', 'predicted_rating'])\n",
    "predictions_output_path = f'./data/output/predicted_ratings_user_{user_id}.csv'\n",
    "predictions_df.to_csv(predictions_output_path, index=False)\n",
    "print(f'Predicted ratings for user {user_id + 1} saved to {predictions_output_path}')\n",
    "\n",
    "comparison_output_path = f'./data/output/rating_comparison_user_{user_id}.csv'\n",
    "create_comparison_csv(mf, user_id, data, n_items, comparison_output_path)\n",
    "mf.export_latent_matrices_and_biases()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
